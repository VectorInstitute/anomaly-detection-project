{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5744b134",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <br><b><font size=\"6\">Unmasking Fraud in Bank Account Openings: <br> A Comprehensive Analysis of the BAF Dataset</font></b><br><br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132aa7f5",
   "metadata": {},
   "source": [
    "Welcome to this notebook where we embark on a comprehensive analysis of the BAF dataset and explore fraud detection in online bank account openings. First, we will delve into a detailed description of the BAF dataset, which centers around detecting fraudulent applications in a large consumer bank. Next, we will dive into the key performance metrics used for evaluating the effectiveness of methods on this dataset. Moreover, we will emphasize the significance of fairness metrics to ensure unbiased decision-making in the high-stakes banking domain. Subsequently, we will introduce the methods employed to address fraud detection, taking into account both performance aspects and fairness considerations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13667c93",
   "metadata": {},
   "source": [
    "# Outline:\n",
    "<br>\n",
    "<ul>\n",
    "    <li><b>Bank Account Fraud (BAF) Dataset</b></li><br>\n",
    "    <li><b>Dataset Preparation</b></li><br>\n",
    "    <li><b>Metrics</b></li><br>\n",
    "    <li><b>Methods</b></li><br>\n",
    "    <li><b>Results</b></li><br>\n",
    "    <li><b>Hyperparameter Tuning</b></li><br>\n",
    "    <li><b>Fraud Detection on Other Variants of BAF Dataset</b></li><br>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9619c778",
   "metadata": {},
   "source": [
    "# Bank Account Fraud (BAF) Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def179e6",
   "metadata": {},
   "source": [
    "![BAF Description](figures/baf_description.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5686307d",
   "metadata": {},
   "source": [
    "The **[BAF dataset](https://arxiv.org/pdf/2211.13358.pdf)** focuses on detecting **fraudulent online bank account opening applications** in a large consumer bank. The scenario involves fraudsters attempting to **impersonate someone or create fictional individuals** to gain access to banking services and carry out illicit activities. The costs incurred by the bank due to these fraudulent activities cannot be traced back to the true identity of the fraudsters. This dataset was accepted at **NeurIPS 2022**.\n",
    "\n",
    "\n",
    "The use case of **fraud detection in the banking domain** is considered **high-stakes**, as positive predictions (flagged as fraudulent) result in the **rejection of customer bank account applications**, while negative predictions grant access to new bank accounts and associated credit cards. As holding a bank account is a basic right in the European Union, fraud detection in this context is of great societal importance. Due to the risk of unfair decision-making using machine learning systems, banks and merchants are at the forefront of adopting fair machine learning methods.\n",
    "\n",
    "The dataset consists of **individual applications**, with each row representing an application made on an online platform. The label indicating fraud or legitimacy is stored in the \"**is_fraud**\" column, where a positive instance represents a fraudulent attempt and a negative instance represents a legitimate application. The dataset spans **eight months** from February to September, with varying **fraud prevalence figures** ranging from **0.85% to 1.5%** across different months. The distribution of applications also varies from month to month, ranging from **9.5% to 15%**.\n",
    "\n",
    "To produce the dataset, a **generative model** is trained with some of original features in the dataset. These features are the top thirty most important features selected from the top-performing LightGBM models, considering expressiveness, interpretability, and redundancy. **Differential privacy** is enforced by perturbing each column in the original dataset using a Laplacian noise mechanism. Additional obfuscation measures are applied to certain applicant data, such as age and income categorization, to enhance privacy.\n",
    "\n",
    "The **CTGAN models** are trained on the perturbed dataset with the selected features, and the dataset is augmented with a column representing the month of application to incorporate temporal information. A total of **70 trained CTGAN models** are created, and the generative models are evaluated based on **predictive performance metrics** and **statistical similarity** between the real and generated data. The goal is to ensure that models trained on the generated datasets are effective at the task and that the generated distribution remains realistic and faithful to the original data.\n",
    "\n",
    "The **performance metric** focuses on defining a relevant threshold and metric for fraud detection, specifically targeting a **5% false positive rate (FPR)** and measuring the **true positive rate (TPR)** at that point. This metric strikes a balance between detecting fraud and minimizing customer attrition. The **fairness metric** aims to ensure that the probability of a legitimate application being wrongly classified as fraudulent is independent of the sensitive attribute value. The **fairness ratio** is calculated by comparing the FPRs of different groups, emphasizing predictive equality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee18857c",
   "metadata": {},
   "source": [
    "![BAF Variants](figures/baf_variants.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94cbe29",
   "metadata": {},
   "source": [
    "The BAF dataset includes **six dataset variants**, each with pre-determined and controllable bias patterns. These variants introduce disparities in group sizes, prevalence, and separability to stress test predictive performance and fairness. The base dataset and its variants are derived from the same underlying distribution, with biases present in the base dataset also present in the variants. The goal is to provide a diverse set of algorithmic fairness challenges for evaluating and improving models' performance and fairness.\n",
    "\n",
    "Overall, the BAF dataset serves as a valuable resource for studying fraud detection in the banking domain, addressing privacy concerns, and exploring the challenges of fairness in machine learning applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e919d4",
   "metadata": {},
   "source": [
    "If you want to know more about the dataset, you can read the data sheet available [here](https://github.com/feedzai/bank-account-fraud/blob/main/documents/datasheet.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e040f2",
   "metadata": {},
   "source": [
    "# Dataset Preparation\n",
    "\n",
    "<p>Now, we want to introduce the <code>BAFDataset</code> class provides a convenient way to load, preprocess, and split the BAF dataset for use in machine learning experiments. The class contains three primary functions that work together to load the data, split it into train and test sets, and preprocess the categorical features via one-hot encoding.</p>\n",
    "\n",
    "<p>Here's a summary of each function within the BAFDataset class:</p>\n",
    "\n",
    "<ul>\n",
    "    <li><code>load_baf(file_path)</code>: This function loads the specified subset of the BAF dataset as a pandas DataFrame. The available subsets are 'Base', 'Variant I', 'Variant II', 'Variant III', 'Variant IV', and 'Variant V'. The function reads the data from a CSV file and drops the 'device_fraud_count' column as it is not needed for further processing.</li>\n",
    "    <li><code>train_test_split(df, month)</code>: This function splits the BAF dataset into train and test sets based on the specified month. The month-based split is inspired by the method proposed in the original paper. Months 0-5 are assigned to the train set, and months 6-7 are assigned to the test set. The function returns a tuple containing the train and test sets.</li>\n",
    "    <li><code>one_hot_encode_categorical(X_train, X_test)</code>: This function preprocesses the categorical features in the BAF dataset by one-hot encoding them. One-hot encoding is a technique that converts categorical features into binary columns for each category/label, making it easier for machine learning models to process the data. The function identifies the categorical features in the dataset, applies one-hot encoding separately for the train and test sets to avoid data leakage, and then combines the transformed data with the numerical features to form the final one-hot encoded train and test sets.</li>\n",
    "\n",
    "</ul>\n",
    "\n",
    "<p>The BAFDataset class streamlines the process of preparing the BAF dataset for machine learning experiments, handling loading, train-test splitting, and preprocessing tasks in a clear and organized manner.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f44294a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary packages and libraries\n",
    "import pandas as pd\n",
    "import lightgbm as lgbm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import xgboost as xgb\n",
    "import catboost as cb\n",
    "import matplotlib.pyplot as plt\n",
    "from baf import BAFDataset\n",
    "from metrics import calculate_tpr_at_fpr, calculate_fairness_metrics\n",
    "import os\n",
    "import joblib\n",
    "import time\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "seed = 100\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "065efc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the BAF class\n",
    "ds = BAFDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21f6d243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the path to the dataset and models\n",
    "\n",
    "base_path = \"/ssd003/projects/aieng/public/anomaly_detection_datasets/BAF\"\n",
    "model_path = \"/ssd003/projects/aieng/public/anomaly_detection_models/BAF\"\n",
    "\n",
    "variant = \"Base.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99e9556c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add file to path\n",
    "file_path = os.path.join(base_path, variant)\n",
    "\n",
    "# Load the dataset\n",
    "df = ds.load_data(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6682e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into train and test sets\n",
    "(X_train, y_train), (X_test, y_test) = ds.train_test_split(df, month=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77b38c51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>income</th>\n",
       "      <th>name_email_similarity</th>\n",
       "      <th>prev_address_months_count</th>\n",
       "      <th>current_address_months_count</th>\n",
       "      <th>customer_age</th>\n",
       "      <th>days_since_request</th>\n",
       "      <th>intended_balcon_amount</th>\n",
       "      <th>payment_type</th>\n",
       "      <th>zip_count_4w</th>\n",
       "      <th>velocity_6h</th>\n",
       "      <th>...</th>\n",
       "      <th>phone_mobile_valid</th>\n",
       "      <th>bank_months_count</th>\n",
       "      <th>has_other_cards</th>\n",
       "      <th>proposed_credit_limit</th>\n",
       "      <th>foreign_request</th>\n",
       "      <th>source</th>\n",
       "      <th>session_length_in_minutes</th>\n",
       "      <th>device_os</th>\n",
       "      <th>keep_alive_session</th>\n",
       "      <th>device_distinct_emails_8w</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>96843</th>\n",
       "      <td>0.8</td>\n",
       "      <td>0.153411</td>\n",
       "      <td>-1</td>\n",
       "      <td>105</td>\n",
       "      <td>30</td>\n",
       "      <td>0.003467</td>\n",
       "      <td>-0.906840</td>\n",
       "      <td>AC</td>\n",
       "      <td>1779</td>\n",
       "      <td>1147.615990</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>0</td>\n",
       "      <td>INTERNET</td>\n",
       "      <td>3.911696</td>\n",
       "      <td>linux</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96844</th>\n",
       "      <td>0.8</td>\n",
       "      <td>0.442439</td>\n",
       "      <td>-1</td>\n",
       "      <td>107</td>\n",
       "      <td>40</td>\n",
       "      <td>0.021131</td>\n",
       "      <td>-1.553115</td>\n",
       "      <td>AC</td>\n",
       "      <td>1195</td>\n",
       "      <td>10788.642570</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>0</td>\n",
       "      <td>INTERNET</td>\n",
       "      <td>23.696737</td>\n",
       "      <td>other</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96845</th>\n",
       "      <td>0.9</td>\n",
       "      <td>0.328328</td>\n",
       "      <td>-1</td>\n",
       "      <td>79</td>\n",
       "      <td>70</td>\n",
       "      <td>0.009110</td>\n",
       "      <td>-0.801146</td>\n",
       "      <td>AC</td>\n",
       "      <td>845</td>\n",
       "      <td>8504.451753</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>INTERNET</td>\n",
       "      <td>2.803927</td>\n",
       "      <td>windows</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96846</th>\n",
       "      <td>0.7</td>\n",
       "      <td>0.033834</td>\n",
       "      <td>-1</td>\n",
       "      <td>28</td>\n",
       "      <td>30</td>\n",
       "      <td>0.004707</td>\n",
       "      <td>-1.171706</td>\n",
       "      <td>AB</td>\n",
       "      <td>1697</td>\n",
       "      <td>6011.217859</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>INTERNET</td>\n",
       "      <td>5.262441</td>\n",
       "      <td>windows</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96847</th>\n",
       "      <td>0.9</td>\n",
       "      <td>0.150643</td>\n",
       "      <td>-1</td>\n",
       "      <td>164</td>\n",
       "      <td>40</td>\n",
       "      <td>0.012221</td>\n",
       "      <td>-0.227823</td>\n",
       "      <td>AB</td>\n",
       "      <td>1474</td>\n",
       "      <td>8378.324044</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>0</td>\n",
       "      <td>INTERNET</td>\n",
       "      <td>5.728984</td>\n",
       "      <td>windows</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       income  name_email_similarity  prev_address_months_count  \\\n",
       "96843     0.8               0.153411                         -1   \n",
       "96844     0.8               0.442439                         -1   \n",
       "96845     0.9               0.328328                         -1   \n",
       "96846     0.7               0.033834                         -1   \n",
       "96847     0.9               0.150643                         -1   \n",
       "\n",
       "       current_address_months_count  customer_age  days_since_request  \\\n",
       "96843                           105            30            0.003467   \n",
       "96844                           107            40            0.021131   \n",
       "96845                            79            70            0.009110   \n",
       "96846                            28            30            0.004707   \n",
       "96847                           164            40            0.012221   \n",
       "\n",
       "       intended_balcon_amount payment_type  zip_count_4w   velocity_6h  ...  \\\n",
       "96843               -0.906840           AC          1779   1147.615990  ...   \n",
       "96844               -1.553115           AC          1195  10788.642570  ...   \n",
       "96845               -0.801146           AC           845   8504.451753  ...   \n",
       "96846               -1.171706           AB          1697   6011.217859  ...   \n",
       "96847               -0.227823           AB          1474   8378.324044  ...   \n",
       "\n",
       "       phone_mobile_valid  bank_months_count  has_other_cards  \\\n",
       "96843                   1                 -1                0   \n",
       "96844                   1                 -1                0   \n",
       "96845                   1                 -1                0   \n",
       "96846                   1                 28                0   \n",
       "96847                   1                  5                0   \n",
       "\n",
       "       proposed_credit_limit foreign_request    source  \\\n",
       "96843                  200.0               0  INTERNET   \n",
       "96844                  200.0               0  INTERNET   \n",
       "96845                 2000.0               0  INTERNET   \n",
       "96846                 1000.0               0  INTERNET   \n",
       "96847                 1500.0               0  INTERNET   \n",
       "\n",
       "       session_length_in_minutes device_os  keep_alive_session  \\\n",
       "96843                   3.911696     linux                   0   \n",
       "96844                  23.696737     other                   0   \n",
       "96845                   2.803927   windows                   0   \n",
       "96846                   5.262441   windows                   1   \n",
       "96847                   5.728984   windows                   0   \n",
       "\n",
       "       device_distinct_emails_8w  \n",
       "96843                          1  \n",
       "96844                          2  \n",
       "96845                          1  \n",
       "96846                          2  \n",
       "96847                          1  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9f5f1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode the categorical features in the dataset\n",
    "X_train, X_test = ds.one_hot_encode_categorical(X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4116c3a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>income</th>\n",
       "      <th>name_email_similarity</th>\n",
       "      <th>prev_address_months_count</th>\n",
       "      <th>current_address_months_count</th>\n",
       "      <th>customer_age</th>\n",
       "      <th>days_since_request</th>\n",
       "      <th>intended_balcon_amount</th>\n",
       "      <th>zip_count_4w</th>\n",
       "      <th>velocity_6h</th>\n",
       "      <th>velocity_24h</th>\n",
       "      <th>...</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>96843</th>\n",
       "      <td>0.8</td>\n",
       "      <td>0.153411</td>\n",
       "      <td>-1</td>\n",
       "      <td>105</td>\n",
       "      <td>30</td>\n",
       "      <td>0.003467</td>\n",
       "      <td>-0.906840</td>\n",
       "      <td>1779</td>\n",
       "      <td>1147.615990</td>\n",
       "      <td>3076.384487</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96844</th>\n",
       "      <td>0.8</td>\n",
       "      <td>0.442439</td>\n",
       "      <td>-1</td>\n",
       "      <td>107</td>\n",
       "      <td>40</td>\n",
       "      <td>0.021131</td>\n",
       "      <td>-1.553115</td>\n",
       "      <td>1195</td>\n",
       "      <td>10788.642570</td>\n",
       "      <td>5790.128646</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96845</th>\n",
       "      <td>0.9</td>\n",
       "      <td>0.328328</td>\n",
       "      <td>-1</td>\n",
       "      <td>79</td>\n",
       "      <td>70</td>\n",
       "      <td>0.009110</td>\n",
       "      <td>-0.801146</td>\n",
       "      <td>845</td>\n",
       "      <td>8504.451753</td>\n",
       "      <td>5912.163054</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96846</th>\n",
       "      <td>0.7</td>\n",
       "      <td>0.033834</td>\n",
       "      <td>-1</td>\n",
       "      <td>28</td>\n",
       "      <td>30</td>\n",
       "      <td>0.004707</td>\n",
       "      <td>-1.171706</td>\n",
       "      <td>1697</td>\n",
       "      <td>6011.217859</td>\n",
       "      <td>5505.127811</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96847</th>\n",
       "      <td>0.9</td>\n",
       "      <td>0.150643</td>\n",
       "      <td>-1</td>\n",
       "      <td>164</td>\n",
       "      <td>40</td>\n",
       "      <td>0.012221</td>\n",
       "      <td>-0.227823</td>\n",
       "      <td>1474</td>\n",
       "      <td>8378.324044</td>\n",
       "      <td>4930.672316</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       income  name_email_similarity  prev_address_months_count  \\\n",
       "96843     0.8               0.153411                         -1   \n",
       "96844     0.8               0.442439                         -1   \n",
       "96845     0.9               0.328328                         -1   \n",
       "96846     0.7               0.033834                         -1   \n",
       "96847     0.9               0.150643                         -1   \n",
       "\n",
       "       current_address_months_count  customer_age  days_since_request  \\\n",
       "96843                           105            30            0.003467   \n",
       "96844                           107            40            0.021131   \n",
       "96845                            79            70            0.009110   \n",
       "96846                            28            30            0.004707   \n",
       "96847                           164            40            0.012221   \n",
       "\n",
       "       intended_balcon_amount  zip_count_4w   velocity_6h  velocity_24h  ...  \\\n",
       "96843               -0.906840          1779   1147.615990   3076.384487  ...   \n",
       "96844               -1.553115          1195  10788.642570   5790.128646  ...   \n",
       "96845               -0.801146           845   8504.451753   5912.163054  ...   \n",
       "96846               -1.171706          1697   6011.217859   5505.127811  ...   \n",
       "96847               -0.227823          1474   8378.324044   4930.672316  ...   \n",
       "\n",
       "        16   17   18   19   20   21   22   23   24   25  \n",
       "96843  0.0  0.0  0.0  1.0  0.0  1.0  0.0  0.0  0.0  0.0  \n",
       "96844  0.0  0.0  0.0  1.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
       "96845  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  1.0  0.0  \n",
       "96846  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  1.0  0.0  \n",
       "96847  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  1.0  0.0  \n",
       "\n",
       "[5 rows x 50 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c45d407",
   "metadata": {},
   "source": [
    "# Metrics\n",
    "\n",
    "\n",
    "\n",
    "![BAF Metrics](figures/baf_metrics.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8041915",
   "metadata": {},
   "source": [
    "Now we want to discuss the metrics we used in evaluating the performance and fairness of the models. The metrics play a crucial role in assessing the effectiveness of the models in detecting fraud and maintaining fairness in the decision-making process.\n",
    "\n",
    "#### `calculate_tpr_at_fpr()`\n",
    "\n",
    "This function calculates the true positive rate (TPR), false positive rate (FPR), and threshold based on a given FPR limit. It takes the predicted scores on the test set, the test set target variable, and the FPR limit as inputs. The function then calculates the ROC curve points and obtains the **threshold and TPR based on the FPR limit**. The TPR represents the proportion of actual positives correctly classified, while the FPR represents the proportion of actual negatives incorrectly classified as positives. These metrics are commonly used in fraud detection to strike a balance between detecting fraud and minimizing customer attrition. The function stores the calculated metrics in a dictionary and returns it.\n",
    "\n",
    "#### `calculate_fairness_metrics()`\n",
    "\n",
    "This function calculates fairness metrics on the predicted scores. It takes the predicted scores on the test set, the test set target variable, and the test set features as inputs. The function initializes a fairness evaluator and calculates the ROC curve points to obtain the threshold. It then creates a dataframe with the scores, labels, and age, and calculates the fairness metrics. The fairness metric in this scenario focuses on ensuring that the probability of a legitimate applicant being wrongly classified as fraudulent is independent of their sensitive attribute value. To assess this, the function calculates the ratio between false positive rates (FPRs) across different groups, referred to as predictive equality. The fairness ratio is obtained by **dividing the FPR of the group with the lowest observed FPR by the FPR of the group with the highest FPR**. This metric helps evaluate and mitigate potential biases in the model's predictions related to sensitive attributes. The function returns the fairness ratio as a measure of fairness.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3586836",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define these dictionaries for storing the results\n",
    "performance_results = {}\n",
    "fairness_results = {}\n",
    "\n",
    "# Dictionaries to store training and inference times\n",
    "training_times_dict = {}\n",
    "inference_times_dict = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eda7b1b",
   "metadata": {},
   "source": [
    "# Methods\n",
    "\n",
    "Now, let's delve into the methods employed to analyze the BAF dataset. The following section will outline the approaches and techniques used to address fraud detection in this high-stakes banking context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660644a5",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "Logistic regression is a popular statistical model used for **binary classification tasks**. It is particularly useful when the **outcome variable is categorical**, with two possible values such as \"yes\" or \"no,\" \"true\" or \"false,\" or **\"normal\"** or **\"anomaly\"**. The goal of logistic regression is to **estimate the probability** that an observation belongs to a certain class based on a set of input features. Mathematically, logistic regression applies the logistic function (also known as the sigmoid function) to a linear combination of input features. This is expressed as:\n",
    "\n",
    "<p align=\"center\">\\[ p(x) = \\frac{1}{1 + e^{-z}} \\]</p>\n",
    "\n",
    "<p>where p(x) is the probability of belonging to a certain class, \\( e \\) is the base of the natural logarithm, and \\( z \\) is the linear combination of input features and their respective coefficients:</p>\n",
    "\n",
    "<p align=\"center\">\\[ z = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + ... + \\beta_nx_n \\]</p>\n",
    "\n",
    "Here, β₀, β₁, β₂, ..., βₙ are the **coefficients** (also known as weights) associated with each input feature x₁, x₂, ..., xₙ. The logistic regression model learns the optimal values of these coefficients by minimizing a cost function, typically the log loss or cross-entropy loss. This process is done through techniques like maximum likelihood estimation or gradient descent. The coefficients obtained from logistic regression represent the **impact of each feature** on the probability of the outcome class. Logistic regression is widely used in various domains, including medicine, finance, and marketing, for tasks such as predicting disease presence, credit risk assessment, and customer churn prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4b6806",
   "metadata": {},
   "source": [
    "Here, we apply Logistic Regression on the data:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88608c6-375d-4a64-b8ed-1b7c234c93d3",
   "metadata": {},
   "source": [
    "Uncomment and run this section and skip the next two cells if you don't want to train the model from scratch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63c4048f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved Logistic Regression model\n",
    "# lr_model_path = os.path.join(model_path, f\"{variant.split('.')[0]}_lr_model.joblib\")\n",
    "# lr_model = joblib.load(lr_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "525e86b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model = LogisticRegression(n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0bc076",
   "metadata": {},
   "source": [
    "<code>class_weight</code> is set to <code>None</code> by default. Try setting it to some weights or <code>balanced</code> mode and see how the performance changes.\n",
    "\n",
    "\n",
    "To calculate class weights, you can use this formula:\n",
    "\n",
    "```python\n",
    "weight_for_class_i = total_samples / (num_samples_in_class_i * num_classes)\n",
    "\n",
    "weight_for_class_0 = len(X_train) / (len(X_train[y_train == 0]) * 2)\n",
    "weight_for_class_1 = len(X_train) / (len(X_train[y_train == 1]) * 2)\n",
    "```\n",
    "\n",
    "Or directly use scikit-learn for this purpose:\n",
    "\n",
    "```python\n",
    "sklearn.utils.class_weight.compute_class_weight(class_weight, classes, y)\n",
    "```\n",
    "\n",
    "After calculating the weights for each class, you can pass them as dictionary to model's fit function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d30b9a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Time: 18.675150394439697 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/ssd004/scratch/vkhazaie/ad/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# Start measuring training time\n",
    "start_time = time.time()\n",
    "\n",
    "# Training the model\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "# End measuring training time\n",
    "end_time = time.time()\n",
    "\n",
    "# Storing and printing the time\n",
    "lr_training_time = end_time - start_time\n",
    "print(f\"Training Time: {lr_training_time} seconds\")\n",
    "training_times_dict[(variant.split('.')[0], \"Logistic Regression\")] = {\"Training Time\":lr_training_time}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca0180af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference Time: 0.08043789863586426 seconds\n"
     ]
    }
   ],
   "source": [
    "# Start measuring inference time\n",
    "start_time = time.time()\n",
    "\n",
    "# predict on the test set\n",
    "lr_scored_test = lr_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# End measuring inference time\n",
    "end_time = time.time()\n",
    "\n",
    "# Storing and printing the time\n",
    "lr_inference_time = end_time - start_time\n",
    "print(f\"Inference Time: {lr_inference_time} seconds\")\n",
    "inference_times_dict[(variant.split('.')[0], \"Logistic Regression\")] = {\"Inference Time\":lr_inference_time}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c004749e",
   "metadata": {},
   "source": [
    "If you want to save the model, you can use:\n",
    "\n",
    "```python\n",
    "import joblib\n",
    "joblib.dump(model, \"PATH_TO_SAVE_MODEL\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "03d30dbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model TPR: 0.2074, Model FPR: 0.0497, Model Threshold: 0.1122, Model AUROC: 0.6802\n"
     ]
    }
   ],
   "source": [
    "# Calculate the TPR@5%FPR on the test set\n",
    "metrics_dict = calculate_tpr_at_fpr(y_test, lr_scored_test, fpr_lim=0.05)\n",
    "print(f\"Model TPR: {metrics_dict['TPR']}, Model FPR: {metrics_dict['FPR']}, Model Threshold: {metrics_dict['Threshold']}, Model AUROC: {metrics_dict['AUROC']}\")\n",
    "performance_results[(variant.split('.')[0], 'Logistic Regression')] = metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "63460e94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fairness Ratio: 0.3455\n"
     ]
    }
   ],
   "source": [
    "# Calculate the fairness metrics on the test set\n",
    "fairness_ratio = calculate_fairness_metrics(y_test, lr_scored_test, X_test, fpr_lim=0.05)\n",
    "fairness_results[(variant.split('.')[0], 'Logistic Regression')] = {\"Fairness Ratio\":fairness_ratio}\n",
    "print(f\"Fairness Ratio: {fairness_ratio}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b10847e",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "\n",
    "Random forest is a powerful ensemble learning algorithm used for both **classification** and **regression** tasks. It combines multiple **decision trees** to make predictions. The 'forest' in random forest refers to a collection of decision trees. Each decision tree in the random forest is built on a random subset of the training data and a random subset of the input features. This randomness helps to create diverse and uncorrelated trees.\n",
    "\n",
    "A decision tree is a popular supervised learning algorithm used for both classification and regression tasks. It creates a flowchart-like model where **each internal node represents a feature**, **each branch represents a decision rule**, and **each leaf node represents the outcome or prediction**.  The decision tree algorithm builds the tree by recursively partitioning the data based on the values of the input features. It selects the best feature and the optimal split point at each step, aiming to maximize the homogeneity of the target variable within each partition. The homogeneity is typically measured using metrics such as **Gini impurity** or information gain. To make a **prediction** using a decision tree, we traverse the tree from the root node to a leaf node based on the feature values of the instance being classified or predicted. The outcome or prediction associated with that leaf node is then assigned to the instance.\n",
    "\n",
    "To make a prediction using a random forest, let's assume we have a random forest with \\(N\\) decision trees. For **classification**, each tree independently predicts the class label using its corresponding subset of features and training samples. The final prediction is determined by **majority voting**, where the class label with the highest count across all trees is selected. For **regression**, each tree independently predicts the target variable, and the final prediction is obtained by **averaging** the predictions of all the trees.\n",
    "\n",
    "<p>Mathematically, let's denote the random forest prediction as \\(y_{\\text{RF}}\\). For classification, the prediction is given by:</p>\n",
    "\n",
    "<p align=\"center\">\\[y_{\\text{RF}} = \\text{argmax}\\left(\\sum_{i=1}^{N} \\mathbb{F}(y_i = c)\\right)\\]</p>\n",
    "\n",
    "<p>where \\(y_i\\) represents the predicted class label of the \\(i\\)th tree, \\(\\mathbb{F}\\) is the indicator function, and \\(c\\) is the class label being considered. For regression, the prediction is given by:</p>\n",
    "\n",
    "<p align=\"center\">\\[y_{\\text{RF}} = \\frac{1}{N}\\sum_{i=1}^{N} y_i\\]</p>\n",
    "\n",
    "<p>where \\(y_i\\) represents the predicted target value of the \\(i\\)th tree.</p>\n",
    "\n",
    "One of the key advantages of random forest is its ability to handle **high-dimensional data** with many input features. It can capture **complex relationships** between features and target variables, handle **missing values** and **outliers**, and provide measures of **feature importance**.\n",
    "\n",
    "The random forest algorithm also addresses the problem of **overfitting**, which can occur with individual decision trees. By combining multiple trees, random forest reduces overfitting and improves generalization performance. Additionally, random forest has mechanisms to estimate the **out-of-bag (OOB) error**, which provides an unbiased estimate of the model's performance without the need for a separate validation set.\n",
    "\n",
    "Random forest has a wide range of applications, including but not limited to, **fraud detection**, **sentiment analysis**, **customer churn prediction**, and **medical diagnosis**. It is a popular and versatile algorithm in machine learning due to its **robustness**, **accuracy**, and **interpretability**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca53916b",
   "metadata": {},
   "source": [
    "Uncomment and run this section and skip the next two cells if you don't want to train the model from scratch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "869da34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved Random Forest model\n",
    "# rf_model_path = os.path.join(model_path, f\"{variant.split('.')[0]}_rf_model.joblib\")\n",
    "# rf_model = joblib.load(rf_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "663ce063",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = RandomForestClassifier(n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7256cf-7933-4d86-be1f-9bb600669915",
   "metadata": {},
   "source": [
    "<code>class_weight</code> is set to <code>None</code> by default. Try setting it to some weights or <code>balanced</code> mode and see how the performance changes.\n",
    "\n",
    "You can either use from sklearn.utils import class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0baa8b98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Time: 283.0788719654083 seconds\n"
     ]
    }
   ],
   "source": [
    "# Start measuring training time\n",
    "start_time = time.time()\n",
    "\n",
    "# Training the model\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# End measuring training time\n",
    "end_time = time.time()\n",
    "\n",
    "# Storing and printing the time\n",
    "rf_training_time = end_time - start_time\n",
    "print(f\"Training Time: {rf_training_time} seconds\")\n",
    "training_times_dict[(variant.split('.')[0], \"Random Forest\")] = {\"Training Time\":rf_training_time}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f677d4f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference Time: 3.608654499053955 seconds\n"
     ]
    }
   ],
   "source": [
    "# Start measuring inference time\n",
    "start_time = time.time()\n",
    "\n",
    "# predict on the test set\n",
    "rf_scored_test = rf_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# End measuring inference time\n",
    "end_time = time.time()\n",
    "\n",
    "# Storing and printing the time\n",
    "rf_inference_time = end_time - start_time\n",
    "print(f\"Inference Time: {rf_inference_time} seconds\")\n",
    "inference_times_dict[(variant.split('.')[0], \"Random Forest\")] = {\"Inference Time\":rf_inference_time}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b0943857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model TPR: 0.4288, Model FPR: 0.0415, Model Threshold: 0.09, Model AUROC: 0.8398\n"
     ]
    }
   ],
   "source": [
    "# Calculate the TPR@5%FPR on the test set\n",
    "metrics_dict = calculate_tpr_at_fpr(y_test, rf_scored_test, fpr_lim=0.05)\n",
    "performance_results[(variant.split('.')[0], 'Random Forest')] = metrics_dict\n",
    "print(f\"Model TPR: {metrics_dict['TPR']}, Model FPR: {metrics_dict['FPR']}, Model Threshold: {metrics_dict['Threshold']}, Model AUROC: {metrics_dict['AUROC']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f4bf0674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fairness Ratio: 0.3007\n"
     ]
    }
   ],
   "source": [
    "# Calculate the fairness metrics on the test set\n",
    "fairness_ratio = calculate_fairness_metrics(y_test, rf_scored_test, X_test, fpr_lim=0.05)\n",
    "fairness_results[(variant.split('.')[0], 'Random Forest')] = {\"Fairness Ratio\":fairness_ratio}\n",
    "print(f\"Fairness Ratio: {fairness_ratio}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c275a86b",
   "metadata": {},
   "source": [
    "## Gradient Boosting \n",
    "\n",
    "Gradient boosting is a powerful machine learning technique used for both regression and classification tasks. It is an ensemble method that combines multiple weak prediction models, typically decision trees, to create a strong predictive model. Unlike random forest, which builds independent trees, gradient boosting builds trees in a sequential manner.\n",
    "\n",
    "The gradient boosting algorithm works by iteratively adding new trees to the model, each one trained to correct the mistakes of the previous trees. At each iteration, the algorithm calculates the gradients of a specified loss function with respect to the predictions made by the existing ensemble. The new tree is then built to minimize this gradient, effectively fitting the residual errors.\n",
    "\n",
    "<p>To make predictions using a gradient boosting model, let's assume we have a gradient boosting model with \\(N\\) trees. The prediction \\(y_{\\text{GB}}\\) is obtained by summing the predictions of all the trees, with each tree weighted by a learning rate \\(\\eta\\):</p>\n",
    "\n",
    "<p align=\"center\">\\[y_{\\text{GB}} = \\sum_{i=1}^{N} \\eta \\cdot h_i(x)\\]</p>\n",
    "\n",
    "<p>where \\(h_i(x)\\) represents the prediction made by the \\(i\\)th tree.</p>\n",
    "\n",
    "One of the key advantages of gradient boosting is its ability to handle complex and nonlinear relationships in the data. It can automatically capture interactions between features and handle missing values. Gradient boosting is also known for its robustness against outliers and its capability to handle high-dimensional data.\n",
    "\n",
    "However, gradient boosting is prone to overfitting if the model becomes too complex or if the learning rate is set too high. Regularization techniques such as limiting the maximum depth of the trees or using early stopping can be employed to prevent overfitting.\n",
    "\n",
    "Gradient boosting has become widely popular and has numerous applications in various domains, including but not limited to, web search ranking, anomaly detection, recommendation systems, and healthcare analytics. It consistently delivers state-of-the-art performance and is favored by many data scientists for its effectiveness and flexibility.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751cc685",
   "metadata": {},
   "source": [
    "### XGBoost\n",
    "\n",
    "**XGBoost** (Extreme Gradient Boosting) is an advanced implementation of gradient boosting, known for its exceptional performance and efficiency. It is widely used in machine learning competitions and has become a popular choice in various data science projects.\n",
    "\n",
    "XGBoost builds upon the principles of gradient boosting and introduces several enhancements to improve the model's predictive power and speed. One of the key innovations is its handling of regularization techniques such as **L1 and L2 regularization**, which help control model complexity and prevent overfitting. By incorporating regularization, XGBoost can effectively handle high-dimensional datasets with many features.\n",
    "\n",
    "Furthermore, XGBoost employs a technique called \"column block\" for parallel computing, making it significantly faster than traditional gradient boosting implementations. It optimizes the computation by dividing the dataset into column blocks, allowing for efficient parallel processing.\n",
    "\n",
    "XGBoost supports both **classification and regression** tasks. For classification, it uses a variant of gradient boosting called \"gradient boosted trees,\" where each tree predicts the class probabilities using a softmax transformation. For regression, XGBoost builds a regression tree ensemble.\n",
    "\n",
    "One of the distinguishing features of XGBoost is its ability to handle **missing values** by automatically learning the best direction to handle them during tree construction. This eliminates the need for imputing missing values before training the model.\n",
    "\n",
    "XGBoost also provides **interpretable model outputs** by ranking the importance of each feature. It calculates feature importance based on the number of times a feature is used in the tree ensemble and the average gain achieved by splits involving that feature.\n",
    "\n",
    "Due to its exceptional performance, scalability, and robustness, XGBoost has gained popularity across various domains, including finance, healthcare, e-commerce, and recommendation systems. It is implemented in multiple programming languages, including Python, R, Java, and Scala, making it easily accessible to data scientists and machine learning practitioners."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d83675",
   "metadata": {},
   "source": [
    "Uncomment and run this section and skip the next two cells if you don't want to train the model from scratch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "215b4348",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved XGBoost model\n",
    "# xgb_model_path = os.path.join(model_path, f\"{variant.split('.')[0]}_xgb_model.joblib\")\n",
    "# xgb_model = joblib.load(xgb_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2e8a78a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost model\n",
    "xgb_model = xgb.XGBClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd236744-a5b2-49d6-9181-1a28053b4a93",
   "metadata": {},
   "source": [
    "One hyperparameter in XGBoost for imbalance classfication is `scale_pos_weight`.\n",
    "\n",
    "By default, `scale_pos_weight` is set to **1.0, which balances positive and negative examples**. XGBoost **minimizes a loss function during training, with gradients reflecting the error magnitude**. A low gradient signifies a small error, and vice versa.\n",
    "\n",
    "The `scale_pos_weight` hyperparameter **scales the gradient related to the positive (minority) class**. This encourages the model to **rectify positive class errors more assertively, potentially enhancing predictions**. However, **excessive scaling can result in overfitting the positive class**, harming performance on both classes.\n",
    "\n",
    "A reasonable default is **the inverse of the class distribution**. For instance, if the minority:majority class ratio is **1:100, setting `scale_pos_weight` to 100 amplifies the impact of minority class errors during training**.\n",
    "\n",
    "Try different values and see how the performance changes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bd30cd48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Time: 307.6446142196655 seconds\n"
     ]
    }
   ],
   "source": [
    "# Start measuring training time\n",
    "start_time = time.time()\n",
    "\n",
    "# Training the model\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# End measuring training time\n",
    "end_time = time.time()\n",
    "\n",
    "# Storing and printing the time\n",
    "xgb_training_time = end_time - start_time\n",
    "print(f\"Training Time: {xgb_training_time} seconds\")\n",
    "training_times_dict[(variant.split('.')[0], \"XGBoost\")] = {\"Training Time\":xgb_training_time}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a88e1ef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference Time: 0.5491247177124023 seconds\n"
     ]
    }
   ],
   "source": [
    "# Start measuring inference time\n",
    "start_time = time.time()\n",
    "\n",
    "# predict on the test set\n",
    "xgb_scored_test = xgb_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# End measuring inference time\n",
    "end_time = time.time()\n",
    "\n",
    "# Storing and printing the time\n",
    "xgb_inference_time = end_time - start_time\n",
    "print(f\"Inference Time: {xgb_inference_time} seconds\")\n",
    "inference_times_dict[(variant.split('.')[0], \"XGBoost\")] = {\"Inference Time\":xgb_inference_time}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "40500131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model TPR: 0.5049, Model FPR: 0.0498, Model Threshold: 0.041099999099969864, Model AUROC: 0.8787\n"
     ]
    }
   ],
   "source": [
    "# Calculate the TPR@5%FPR on the test set\n",
    "metrics_dict = calculate_tpr_at_fpr(y_test, xgb_scored_test, fpr_lim=0.05)\n",
    "performance_results[(variant.split('.')[0], 'XGBoost')] = metrics_dict\n",
    "print(f\"Model TPR: {metrics_dict['TPR']}, Model FPR: {metrics_dict['FPR']}, Model Threshold: {metrics_dict['Threshold']}, Model AUROC: {metrics_dict['AUROC']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3baf2148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fairness Ratio: 0.3233\n"
     ]
    }
   ],
   "source": [
    "# Calculate the fairness metrics on the test set\n",
    "fairness_ratio = calculate_fairness_metrics(y_test, xgb_scored_test, X_test, fpr_lim=0.05)\n",
    "fairness_results[(variant.split('.')[0], 'XGBoost')] = {\"Fairness Ratio\":fairness_ratio}\n",
    "print(f\"Fairness Ratio: {fairness_ratio}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003a8f1f",
   "metadata": {},
   "source": [
    "### CatBoost\n",
    "\n",
    "**CatBoost** is a powerful gradient boosting framework developed by Yandex that is particularly well-suited for working with **categorical features**. It is designed to handle the inherent challenges posed by categorical variables, such as high cardinality and missing values.\n",
    "\n",
    "One of the key advantages of CatBoost is its ability to automatically handle categorical features without requiring explicit preprocessing. It employs an innovative technique called **ordered boosting**, which utilizes a novel algorithm to process categorical variables directly, eliminating the need for one-hot encoding or label encoding. This simplifies the feature engineering process and helps capture valuable information present in categorical features.\n",
    "\n",
    "Additionally, CatBoost implements **gradient-based optimization** to handle numerical features effectively. It leverages the power of gradient boosting to learn complex non-linear relationships and interactions between features, resulting in accurate predictions.\n",
    "\n",
    "CatBoost also includes built-in **robustness** mechanisms to handle missing data in both categorical and numerical features. It can automatically handle missing values during training, reducing the need for imputation.\n",
    "\n",
    "The framework supports various advanced features, such as **regularization**, **cross-validation**, and **early stopping**, which enhance model generalization and prevent overfitting. CatBoost also provides **interpretability** by offering feature importance analysis and visualization tools to understand the impact of different features on the predictions.\n",
    "\n",
    "Furthermore, CatBoost is known for its high performance and scalability. It implements **optimized algorithms** to process large datasets efficiently, making it suitable for both small-scale and large-scale applications.\n",
    "\n",
    "CatBoost is available in multiple programming languages, including Python, R, and Julia, making it accessible to a wide range of users. It integrates well with popular data science libraries and frameworks, enabling seamless integration into existing workflows.\n",
    "\n",
    "Due to its unique handling of categorical features, robustness, and performance, CatBoost has gained popularity in various domains, including advertising, recommendation systems, and fraud detection. It is widely used by data scientists and machine learning practitioners to achieve accurate predictions with minimal feature engineering efforts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e884c58",
   "metadata": {},
   "source": [
    "Uncomment and run this section and skip the next two cells if you don't want to train the model from scratch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0d663390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved CatBoost model\n",
    "# cb_model_path = os.path.join(model_path, f\"{variant.split('.')[0]}_cb_model.joblib\")\n",
    "# cb_model = joblib.load(cb_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7e91e8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CatBoost model\n",
    "cb_model = cb.CatBoostClassifier(verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65f045d-0bc7-4007-a395-74ea1fb48c60",
   "metadata": {},
   "source": [
    "CatBoost offers two options, `scale_pos_weight` and `class_weights`, to handle imbalanced classification. You have the choice to utilize either of them. Experiment with various values for these options to observe the resulting performance changes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9bf57c84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TBB Warning: The number of workers is currently limited to 1. The request for 31 workers is ignored. Further requests for more workers will be silently ignored until the limit changes.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Time: 198.03822469711304 seconds\n"
     ]
    }
   ],
   "source": [
    "# Start measuring training time\n",
    "start_time = time.time()\n",
    "\n",
    "# Training the model\n",
    "cb_model.fit(X_train, y_train)\n",
    "\n",
    "# End measuring training time\n",
    "end_time = time.time()\n",
    "\n",
    "# Storing and printing the time\n",
    "cb_training_time = end_time - start_time\n",
    "print(f\"Training Time: {cb_training_time} seconds\")\n",
    "training_times_dict[(variant.split('.')[0], \"CatBoost\")] = {\"Training Time\":cb_training_time}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7c6f462b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference Time: 0.3733079433441162 seconds\n"
     ]
    }
   ],
   "source": [
    "# Start measuring inference time\n",
    "start_time = time.time()\n",
    "\n",
    "# predict on the test set\n",
    "cb_scored_test = cb_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# End measuring inference time\n",
    "end_time = time.time()\n",
    "\n",
    "# Storing and printing the time\n",
    "cb_inference_time = end_time - start_time\n",
    "print(f\"Inference Time: {cb_inference_time} seconds\")\n",
    "inference_times_dict[(variant.split('.')[0], \"CatBoost\")] = {\"Inference Time\":cb_inference_time}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "98fdb28b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model TPR: 0.5101, Model FPR: 0.0499, Model Threshold: 0.0373, Model AUROC: 0.8836\n"
     ]
    }
   ],
   "source": [
    "# Calculate the TPR@5%FPR on the test set\n",
    "metrics_dict = calculate_tpr_at_fpr(y_test, cb_scored_test, fpr_lim=0.05)\n",
    "performance_results[(variant.split('.')[0], 'CatBoost')] = metrics_dict\n",
    "print(f\"Model TPR: {metrics_dict['TPR']}, Model FPR: {metrics_dict['FPR']}, Model Threshold: {metrics_dict['Threshold']}, Model AUROC: {metrics_dict['AUROC']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "208d83ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fairness Ratio: 0.2992\n"
     ]
    }
   ],
   "source": [
    "# Calculate the fairness metrics on the test set\n",
    "fairness_ratio = calculate_fairness_metrics(y_test, cb_scored_test, X_test, fpr_lim=0.05)\n",
    "fairness_results[(variant.split('.')[0], 'CatBoost')] = {\"Fairness Ratio\":fairness_ratio}\n",
    "print(f\"Fairness Ratio: {fairness_ratio}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1bf41e-97d3-4149-b8e1-4a8388054831",
   "metadata": {},
   "source": [
    "### LightGBM\n",
    "\n",
    "**Light GBM** (Light Gradient Boosting Machine) is a gradient boosting framework that offers high performance and efficiency for machine learning tasks. It is designed to handle large-scale datasets and has gained popularity for its speed and accuracy.\n",
    "\n",
    "Light GBM leverages a unique approach to gradient boosting by using a technique called **gradient-based one-sided sampling**. This approach selects only the most informative data instances for building each tree, reducing the computational resources required while maintaining predictive accuracy.\n",
    "\n",
    "Another important feature of Light GBM is its ability to handle **categorical features** directly. It converts categorical variables into numerical values, which improves the model's performance and reduces the need for manual feature engineering.\n",
    "\n",
    "Light GBM employs a **leaf-wise tree growth** strategy, where each tree is grown leaf-wise instead of level-wise. This strategy focuses on growing the leaves that contribute the most to the overall objective function, leading to faster convergence and improved efficiency.\n",
    "\n",
    "The framework also supports various advanced techniques, such as **regularization** (e.g., L1 and L2 regularization), **early stopping**, and **data parallelism**. These techniques enhance model generalization, prevent overfitting, and enable efficient distributed training.\n",
    "\n",
    "Light GBM is implemented in multiple programming languages, including Python, R, and C++. It provides a user-friendly interface and is compatible with popular machine learning libraries like scikit-learn.\n",
    "\n",
    "Due to its speed, scalability, and performance, Light GBM is widely used in various domains, including online advertising, recommendation systems, and fraud detection. It has become a go-to choice for data scientists and machine learning practitioners when working with large datasets and requiring fast and accurate models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b302e7e5-284a-447f-88f8-d8b7f2d4752d",
   "metadata": {},
   "source": [
    "Uncomment and run this section and skip the next two cells if you don't want to train the model from scratch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "8a76c4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved Light GBM model\n",
    "# lgbm_model_path = os.path.join(model_path, f\"{variant.split('.')[0]}_lgbm_model.joblib\")\n",
    "# lgbm_model = joblib.load(lgbm_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9af11e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a model on the data\n",
    "lgbm_model = lgbm.LGBMClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b5d80b-405e-49d0-a60b-e2aa89994c13",
   "metadata": {},
   "source": [
    "<code>class_weight</code> is set to <code>None</code> by default. Try setting it to some weights or <code>balanced</code> mode and see how the performance changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e0d3ce11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 8151, number of negative: 786838\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.159283 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 794989, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569882\n",
      "[LightGBM] [Info] Start training from score -4.569882\n",
      "Training Time: 13.89889645576477 seconds\n"
     ]
    }
   ],
   "source": [
    "# Start measuring training time\n",
    "start_time = time.time()\n",
    "\n",
    "# Training the model\n",
    "lgbm_model.fit(X_train, y_train)\n",
    "\n",
    "# End measuring training time\n",
    "end_time = time.time()\n",
    "\n",
    "# Storing and printing the time\n",
    "lgbm_training_time = end_time - start_time\n",
    "print(f\"Training Time: {lgbm_training_time} seconds\")\n",
    "training_times_dict[(variant.split('.')[0], \"Light GBM\")] = {\"Training Time\":lgbm_training_time}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b688f2bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference Time: 1.0104155540466309 seconds\n"
     ]
    }
   ],
   "source": [
    "# Start measuring inference time\n",
    "start_time = time.time()\n",
    "\n",
    "# predict on the test set\n",
    "lgbm_scored_test = lgbm_model.predict_proba(X_test)[:,1]\n",
    "\n",
    "# End measuring inference time\n",
    "end_time = time.time()\n",
    "\n",
    "# Storing and printing the time\n",
    "lgbm_inference_time = end_time - start_time\n",
    "print(f\"Inference Time: {lgbm_inference_time} seconds\")\n",
    "inference_times_dict[(variant.split('.')[0], \"Light GBM\")] = {\"Inference Time\":lgbm_inference_time}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "95c9dbf5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model TPR: 0.5302, Model FPR: 0.05, Model Threshold: 0.0404, Model AUROC: 0.8898\n"
     ]
    }
   ],
   "source": [
    "# Calculate the  TPR@5%FPR on the test set\n",
    "metrics_dict = calculate_tpr_at_fpr(y_test, lgbm_scored_test, fpr_lim=0.05)\n",
    "performance_results[(variant.split('.')[0], 'Light GBM')] = metrics_dict\n",
    "print(f\"Model TPR: {metrics_dict['TPR']}, Model FPR: {metrics_dict['FPR']}, Model Threshold: {metrics_dict['Threshold']}, Model AUROC: {metrics_dict['AUROC']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1d899670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fairness Ratio: 0.3017\n"
     ]
    }
   ],
   "source": [
    "# Calculate the fairness metrics on the test set\n",
    "fairness_ratio = calculate_fairness_metrics(y_test, lgbm_scored_test, X_test, fpr_lim=0.05)\n",
    "fairness_results[(variant.split('.')[0], 'Light GBM')] = {\"Fairness Ratio\":fairness_ratio}\n",
    "print(f\"Fairness Ratio: {fairness_ratio}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defa525d",
   "metadata": {},
   "source": [
    "## Comparison of Different Gradient Boosting Libraries (XGBoost, CatBoost and Light GBM)\n",
    "\n",
    "### Differences:\n",
    "- **Feature Handling**: XGBoost requires explicit preprocessing of categorical features, while CatBoost natively handles them, and LightGBM uses binning techniques for efficient categorical feature handling.\n",
    "- **Performance and Efficiency**: XGBoost is known for its scalability and optimization, CatBoost emphasizes memory efficiency, and LightGBM offers faster training speed and lower memory usage.\n",
    "- **Parallelization and Distributed Computing**: All three frameworks support parallelization, but XGBoost and CatBoost also provide options for distributed computing.\n",
    "- **Implementation and Community Support**: XGBoost has a longer history, a mature implementation, and a large user community. CatBoost and LightGBM are newer but gaining popularity, with active development communities and good documentation.\n",
    "\n",
    "### Similarities:\n",
    "- **Gradient Boosting**: All three frameworks are based on the gradient boosting technique.\n",
    "- **Ensemble Learning**: They create ensembles by combining multiple weak models (decision trees) for improved predictive power.\n",
    "- **Anomaly Detection**: All three can be used for anomaly detection tasks.\n",
    "- **Widely Used**: XGBoost, CatBoost, and LightGBM are widely used in various domains and have demonstrated success in many machine learning applications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f708c70b",
   "metadata": {},
   "source": [
    "## TabNet: Attentive Interpretable Tabular Learning\n",
    "\n",
    "**TabNet** presents an innovative approach that bridges the gap between two prominent machine learning paradigms: **Decision Trees (DTs)** and **Deep Neural Networks (DNNs)**. The methodology introduced in this work combines the strengths of these two approaches to create a powerful learning framework for real-world **tabular datasets**. While **DNNs lack the transparency and interpretability of DTs**, DTs often struggle to capture the intricacies of complex patterns present in diverse tabular data. TabNet's primary contribution revolves around its meticulously designed architecture, which effectively integrates the output structure of DTs within the flexible framework of DNNs.\n",
    "\n",
    "The central concept driving TabNet is the emulation of **decision boundaries** akin to those established by DTs. However, TabNet introduces a novel mechanism by leveraging a sequence of **learnable masks** to guide feature selection at each step. These masks, derived through the innovative **sparsemax normalization technique**, offer a dual advantage: they optimize parameter usage efficiently and promote interpretability by **highlighting the most pertinent features**. Moreover, the incorporation of **sparsity regularization** empowers the model with the ability to control the extent of feature selection, a crucial capability when dealing with datasets containing redundant or irrelevant features.\n",
    "\n",
    "TabNet's architecture excels due to its **sequential multi-step approach**, wherein decisions are iteratively refined by progressively considering the most influential features. Inspired by applications in visual processing and reinforcement learning, the **top-down attention mechanism** equips the model to concentrate on relevant information, effectively tackling the challenges posed by high-dimensional inputs. This results in a dynamic decision process that effectively harnesses the strengths of both local and global features, thereby enhancing TabNet's proficiency in capturing intricate relationships within tabular data.\n",
    "\n",
    "In essence, TabNet strikes a remarkable balance between **predictive power and interpretability**. Its capability to not only **outperform traditional DTs** but also provide **invaluable insights into feature importance** positions it as a potent solution for learning from tabular datasets. This becomes particularly relevant in domains where comprehending the decision-making process is just as pivotal as achieving accurate predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d9118cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "from pytorch_tabnet.metrics import Metric\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "from pytorch_tabnet.pretraining import TabNetPretrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bf8e2c66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/ssd004/scratch/vkhazaie/ad/lib/python3.10/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of TabNet model\n",
    "clf = TabNetClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a9e1fb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom metric class to pass to TabNet\n",
    "class TPR_at_05FPR(Metric):\n",
    "    def __init__(self):\n",
    "        self._name = \"tpr_at_05fpr\"\n",
    "        self._maximize = True\n",
    "\n",
    "    def __call__(self, y_true, y_score):\n",
    "        fpr, tpr, threshold = roc_curve(y_true, y_score[:, 1])\n",
    "        tpr_at_05fpr = tpr[fpr < 0.05][-1]\n",
    "        return tpr_at_05fpr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929c98b0",
   "metadata": {},
   "source": [
    "Set 1 for automatic weight balancing or try different values and see how the performance changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "07d244a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/ssd004/scratch/vkhazaie/ad/lib/python3.10/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.05198 |  0:01:05s\n",
      "epoch 1  | loss: 0.04631 |  0:02:11s\n",
      "epoch 2  | loss: 0.04549 |  0:03:19s\n",
      "epoch 3  | loss: 0.04523 |  0:04:26s\n",
      "epoch 4  | loss: 0.04475 |  0:05:32s\n",
      "Training Time: 666.0116283893585 seconds\n"
     ]
    }
   ],
   "source": [
    "# Start measuring training time\n",
    "start_time = time.time()\n",
    "\n",
    "# Train TabNet\n",
    "clf.fit(\n",
    "    X_train.values, \n",
    "    y_train.values,\n",
    "    eval_metric=[TPR_at_05FPR, \"auc\"],\n",
    "    max_epochs=5,\n",
    "    batch_size=512,\n",
    "    weights=0\n",
    ")\n",
    "\n",
    "# End measuring training time\n",
    "end_time = time.time()\n",
    "\n",
    "# Storing and printing the time\n",
    "tabnet_training_time = end_time - start_time\n",
    "print(f\"Training Time: {tabnet_training_time} seconds\")\n",
    "training_times_dict[(variant.split('.')[0], \"TabNet\")] = {\"Training Time\":tabnet_training_time}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c7188f46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference Time: 6.612230062484741 seconds\n"
     ]
    }
   ],
   "source": [
    "# Start measuring inference time\n",
    "start_time = time.time()\n",
    "\n",
    "# Predict on the test set\n",
    "tabnet_scored_test = clf.predict_proba(X_test.values)[:,1]\n",
    "\n",
    "# End measuring inference time\n",
    "end_time = time.time()\n",
    "\n",
    "# Storing and printing the time\n",
    "tabnet_inference_time = end_time - start_time\n",
    "print(f\"Inference Time: {tabnet_inference_time} seconds\")\n",
    "inference_times_dict[(variant.split('.')[0], \"TabNet\")] = {\"Inference Time\":tabnet_inference_time}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c210279",
   "metadata": {},
   "source": [
    "If you want to save the model, you can use:\n",
    "\n",
    "```python\n",
    "clf.save_model(\"PATH_TO_SAVE_MODEL\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "31d44222",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model TPR: 0.5069, Model FPR: 0.05, Model Threshold: 0.028200000524520874, Model AUROC: 0.8822\n"
     ]
    }
   ],
   "source": [
    "# Calculate the TPR@5%FPR on the test set\n",
    "metrics_dict = calculate_tpr_at_fpr(y_test, tabnet_scored_test, fpr_lim=0.05)\n",
    "performance_results[(variant.split('.')[0], 'TabNet')] = metrics_dict\n",
    "print(f\"Model TPR: {metrics_dict['TPR']}, Model FPR: {metrics_dict['FPR']}, Model Threshold: {metrics_dict['Threshold']}, Model AUROC: {metrics_dict['AUROC']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bd07e25c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fairness Ratio: 0.3459 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate the fairness metrics on the test set\n",
    "fairness_ratio = calculate_fairness_metrics(y_test, tabnet_scored_test, X_test, fpr_lim=0.05)\n",
    "fairness_results[(variant.split('.')[0], 'TabNet')] = {\"Fairness Ratio\":fairness_ratio}\n",
    "print(f\"Fairness Ratio: {fairness_ratio}\", \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a36e98",
   "metadata": {},
   "source": [
    "## Autoencoder\n",
    "\n",
    "Now we want to apply some unsupervised methods for anomaly detection. In this cell, we begin by importing essential libraries required for building and training neural networks, as well as for data preprocessing and evaluation. We also import Scikit-learn libraries for metrics calculation and data scaling.\n",
    "\n",
    "Following the imports, we define the architecture of the autoencoder model. An autoencoder is a type of neural network consisting of an encoder and a decoder. The purpose of an autoencoder is to learn a compressed representation of input data and then reconstruct the original data from this representation.\n",
    "\n",
    "We create the `Autoencoder` class as a subclass of `nn.Module`, which is PyTorch's base class for defining neural network modules. The class includes methods for initializing the architecture and performing forward passes.\n",
    "\n",
    "The encoder is structured using `nn.Sequential`, which is a convenient way to define a sequence of layers. It starts with a linear layer that reduces the input data's dimensionality to the specified `hidden_size`, followed by a rectified linear unit (ReLU) activation function to introduce non-linearity.\n",
    "\n",
    "The decoder, resembling the encoder but in reverse order, takes the encoded representation and aims to reconstruct the original input data. The simplicity of this architecture allows for easy experimentation with various configurations and layer sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "df5a0741",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Autoencoder architecture\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(hidden_size, input_size),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc458c9a",
   "metadata": {},
   "source": [
    "In this cell, we set the hyperparameters that will govern the training process of the autoencoder. Hyperparameters are configuration choices that influence the behavior of the neural network during training. It's important to choose appropriate hyperparameters to ensure effective learning and convergence.\n",
    "\n",
    "We define the `input_size` as the number of features in the input data and `hidden_size` as the number of neurons in the hidden layer of the autoencoder. These values can be adjusted based on the complexity of your data and the desired capacity of the model.\n",
    "\n",
    "Other hyperparameters include the `batch_size`, which determines the number of data samples processed in each training iteration, the `learning_rate`, which controls the step size during optimization, and the `num_epochs`, which specifies the number of times the entire training dataset is passed through the model.\n",
    "\n",
    "With the hyperparameters set, we create an instance of the `Autoencoder` class that we defined earlier. This instance represents our neural network model, which will be trained to learn a compressed representation of the input data.\n",
    "\n",
    "We also define the loss function and optimizer. The loss function (`nn.MSELoss()`) calculates the mean squared error between the reconstructed data and the original data. The optimizer (`optim.Adam()`) implements the Adam optimization algorithm, which updates the model's parameters to minimize the loss during training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3bfbf494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "input_size = 50  # Number of input features\n",
    "hidden_size = 30  # Number of neurons in the hidden layer\n",
    "batch_size = 512\n",
    "learning_rate = 0.0001\n",
    "num_epochs = 5\n",
    "\n",
    "# Create an instance of the Autoencoder\n",
    "autoencoder = Autoencoder(input_size, hidden_size)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(autoencoder.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ce53f1",
   "metadata": {},
   "source": [
    "In this cell, we convert the DataFrames to PyTorch tensors using the `torch.tensor` function. This conversion prepares the data for feeding into the neural network model.\n",
    "\n",
    "With the data in the form of PyTorch tensors, we are now ready to proceed with training the autoencoder model in subsequent cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b9d94bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert DataFrame to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b7bf10",
   "metadata": {},
   "source": [
    "In this cell, we begin the training process of the autoencoder neural network. Training a neural network involves iteratively adjusting its parameters to minimize a chosen loss function and improve its performance on the given data.\n",
    "\n",
    "We create a `train_dataset` using the scaled training data, and a `train_dataloader` using `TensorDataset` and `DataLoader` from PyTorch. The dataloader allows us to efficiently iterate over batches of data during training.\n",
    "\n",
    "We initiate a loop that runs for a specified number of epochs. In each epoch, the autoencoder processes batches of training data through the network and updates its parameters to reduce the loss. The loss is calculated using the mean squared error (`MSELoss`) between the input data and the reconstructed output.\n",
    "\n",
    "The `optimizer` then applies gradient descent to adjust the neural network's weights and biases, moving in the direction that minimizes the loss. This process iterates over multiple epochs, gradually improving the autoencoder's ability to capture useful features and create a compressed representation of the data.\n",
    "\n",
    "At the end of each epoch, we print the loss to monitor the training progress and observe how it decreases over time. This cell establishes the core training loop, and the subsequent cells will focus on evaluating the trained autoencoder's performance and analyzing its results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "773262ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 107664.4766\n",
      "Epoch [2/5], Loss: 30822.2695\n",
      "Epoch [3/5], Loss: 6609.6665\n",
      "Epoch [4/5], Loss: 853.1996\n",
      "Epoch [5/5], Loss: 538.7800\n",
      "Training Time: 50.65826106071472 seconds\n"
     ]
    }
   ],
   "source": [
    "# Create DataLoader for batching\n",
    "train_dataset = TensorDataset(X_train_tensor)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "# Start measuring training time\n",
    "start_time = time.time()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = autoencoder(batch[0])\n",
    "        loss = criterion(outputs, batch[0])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "# End measuring training time\n",
    "end_time = time.time()\n",
    "\n",
    "# Storing and printing the time\n",
    "ae_training_time = end_time - start_time\n",
    "print(f\"Training Time: {ae_training_time} seconds\")\n",
    "training_times_dict[(variant.split('.')[0], \"AE\")] = {\"Training Time\":ae_training_time}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9cd662",
   "metadata": {},
   "source": [
    "In this cell, we evaluate the performance of the trained autoencoder for anomaly detection on the test data. Anomaly detection involves identifying instances that significantly deviate from the norm in the dataset.\n",
    "\n",
    "We pass the scaled test data through the trained autoencoder's decoder to obtain reconstructed data. The difference between the original test data and the reconstructed data is used to calculate the reconstruction errors. These errors represent how well the autoencoder is able to recreate the input data.\n",
    "\n",
    "To calculate the reconstruction errors, we use the mean squared error (`MSELoss`) with the reduction set to `'none'`. This means that the reconstruction error is computed for each individual data point without aggregating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ead12a5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference Time: 0.11012578010559082 seconds\n",
      "Model TPR: 0.0629, Model FPR: 0.0498, Model Threshold: 1837.91796875, Model AUROC: 0.6143\n",
      "Fairness Ratio: 0.3692 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Start measuring inference time\n",
    "start_time = time.time()\n",
    "\n",
    "# Pass the test data through the autoencoder\n",
    "with torch.no_grad():\n",
    "    reconstructed_data = autoencoder(X_test_tensor)\n",
    "\n",
    "# End measuring inference time\n",
    "end_time = time.time()\n",
    "\n",
    "# Storing and printing the time\n",
    "ae_inference_time = end_time - start_time\n",
    "print(f\"Inference Time: {ae_inference_time} seconds\")\n",
    "inference_times_dict[(variant.split('.')[0], \"AE\")] = {\"Inference Time\":ae_inference_time}\n",
    "\n",
    "# Calculate reconstruction errors\n",
    "reconstruction_errors = ((reconstructed_data - X_test_tensor)**2).mean(dim=1)\n",
    "\n",
    "# Calculate the TPR@5%FPR on the test set\n",
    "metrics_dict = calculate_tpr_at_fpr(y_test.values, reconstruction_errors.numpy(), fpr_lim=0.05)\n",
    "performance_results[(variant.split('.')[0], 'AE')] = metrics_dict\n",
    "print(f\"Model TPR: {metrics_dict['TPR']}, Model FPR: {metrics_dict['FPR']}, Model Threshold: {metrics_dict['Threshold']}, Model AUROC: {metrics_dict['AUROC']}\")\n",
    "\n",
    "# Calculate the fairness metrics on the test set\n",
    "fairness_ratio = calculate_fairness_metrics(y_test, reconstruction_errors.numpy(), X_test, fpr_lim=0.05)\n",
    "fairness_results[(variant.split('.')[0], 'AE')] = {\"Fairness Ratio\":fairness_ratio}\n",
    "print(f\"Fairness Ratio: {fairness_ratio}\", \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce58ae6",
   "metadata": {},
   "source": [
    "## Isolation Forest\n",
    "\n",
    "The Isolation Forest (iForest) is an **established method** for **spotting anomalies** within datasets. It constructs a **forest of isolation trees** where data points are partitioned based on **random feature choices and values**. Anomalies are identified as points that require **fewer splits to become isolated**, assuming they are **distinct from the majority of data points**. This approach is **effective for simple anomalies** but **struggles when anomalies are complex, high-dimensional, or situated in non-linear data configurations**.\n",
    "\n",
    "In this cell, we utilize Scikit-learn's Isolation Forest algorithm for anomaly detection. The Isolation Forest algorithm excels in identifying anomalies by isolating them within a tree-based structure. The `IsolationForest` class is imported from the `ensemble` module.\n",
    "\n",
    "We set the `n_jobs` parameter to -1, which enables the algorithm to leverage all available CPU cores for parallel computations, potentially accelerating the training process. Additionally, we establish a specific random seed, denoted as `seed`, to ensure the reproducibility of results.\n",
    "\n",
    "The Isolation Forest is trained on the `X_train` dataset, containing the feature values of the training instances. Once trained, we compute anomaly scores for the test data using the `decision_function` method. These scores quantify the extent to which each test instance deviates from the norm, where higher scores indicate a higher probability of being an anomaly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c72f620d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Time: 0.9152803421020508 seconds\n",
      "Inference Time: 1.4858503341674805 seconds\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# Train Isolation Forest\n",
    "isolation_forest = IsolationForest(n_jobs=-1, random_state=seed)\n",
    "\n",
    "\n",
    "# Start measuring training time\n",
    "start_time = time.time()\n",
    "\n",
    "# Train\n",
    "isolation_forest.fit(X_train)\n",
    "\n",
    "# End measuring training time\n",
    "end_time = time.time()\n",
    "\n",
    "# Storing and printing the time\n",
    "if_training_time = end_time - start_time\n",
    "print(f\"Training Time: {if_training_time} seconds\")\n",
    "training_times_dict[(variant.split('.')[0], \"IF\")] = {\"Training Time\":if_training_time}\n",
    "\n",
    "\n",
    "# Start measuring inference time\n",
    "start_time = time.time()\n",
    "\n",
    "# Predict\n",
    "isolation_scores = isolation_forest.decision_function(X_test)\n",
    "\n",
    "# End measuring inference time\n",
    "end_time = time.time()\n",
    "\n",
    "# Storing and printing the time\n",
    "if_inference_time = end_time - start_time\n",
    "print(f\"Inference Time: {if_inference_time} seconds\")\n",
    "inference_times_dict[(variant.split('.')[0], \"IF\")] = {\"Inference Time\":if_inference_time}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bde13fc",
   "metadata": {},
   "source": [
    "This cell assesses the performance of the trained Isolation Forest model for anomaly detection. The objective is to gauge its capacity to accurately detect anomalies while managing false positives.\n",
    "\n",
    "In Isolation Forest, the anomaly scores provided by the decision_function method are calculated as the average path length of an instance in the isolation tree forest. An instance with a shorter average path length is considered more anomalous. However, in terms of interpreting the scores, lower values are more indicative of anomalies. Anomalies are expected to have fewer average path lengths due to their isolated nature.\n",
    "\n",
    "By negating the scores, you effectively transform the interpretation so that higher negated scores represent higher anomaly likelihood. This aligns better with the common understanding that higher scores indicate higher degrees of anomalous behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "8c96e285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model TPR: 0.0806, Model FPR: 0.0499, Model Threshold: 0.0155, Model AUROC: 0.5816\n",
      "Fairness Ratio: 0.2516 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate TPR@5%FPR for Isolation Forest\n",
    "metrics_dict = calculate_tpr_at_fpr(y_test.values, -isolation_scores, fpr_lim=0.05)\n",
    "performance_results[(variant.split('.')[0], 'IF')] = metrics_dict\n",
    "print(f\"Model TPR: {metrics_dict['TPR']}, Model FPR: {metrics_dict['FPR']}, Model Threshold: {metrics_dict['Threshold']}, Model AUROC: {metrics_dict['AUROC']}\")\n",
    "\n",
    "# Calculate fairness metrics for Isolation Forest\n",
    "fairness_ratio = calculate_fairness_metrics(y_test.values, -isolation_scores, X_test, fpr_lim=0.05)\n",
    "fairness_results[(variant.split('.')[0], 'IF')] = {\"Fairness Ratio\":fairness_ratio}\n",
    "print(f\"Fairness Ratio: {fairness_ratio}\", \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df967e0e",
   "metadata": {},
   "source": [
    "## DeepOD: Deep Learning-Based Anomaly Detection Framework\n",
    "\n",
    "DeepOD is an open-source Python framework designed for deep learning-based anomaly detection. It offers a unified implementation of various detection models based on PyTorch.\n",
    "\n",
    "DeepOD currently encompasses 13 deep anomaly detection algorithms in the unsupervised and weakly-supervised paradigms. The framework provides a platform for easy experimentation with these models, and additional baseline algorithms will be incorporated in the future.\n",
    "\n",
    "**Detection Models:**\n",
    "\n",
    "The following detection models are included in DeepOD:\n",
    "\n",
    "| Model       | Venue | Year | Type            | Title\n",
    "|-------------|-------|------|-----------------|----------------------------------------------------\n",
    "| Deep SVDD   | ICML  | 2018 | Unsupervised    | Deep One-Class Classification\n",
    "| REPEN       | KDD   | 2018 | Unsupervised    | Learning Representations of Ultrahigh-dimensional Data for Random Distance-based Outlier Detection\n",
    "| RDP         | IJCAI | 2020 | Unsupervised    | Unsupervised Representation Learning by Predicting Random Distances\n",
    "| RCA         | IJCAI | 2021 | Unsupervised    | RCA: A Deep Collaborative Autoencoder Approach for Anomaly Detection\n",
    "| GOAD        | ICLR  | 2020 | Unsupervised    | Classification-Based Anomaly Detection for General Data\n",
    "| NeuTraL     | ICML  | 2021 | Unsupervised    | Neural Transformation Learning for Deep Anomaly Detection Beyond Images\n",
    "| ICL         | ICLR  | 2022 | Unsupervised    | Anomaly Detection for Tabular Data with Internal Contrastive Learning\n",
    "| DIF         | TKDE  | 2023 | Unsupervised    | Deep Isolation Forest for Anomaly Detection\n",
    "| SLAD        | ICML  | 2023 | Unsupervised    | Fascinating Supervisory Signals and Where to Find Them: Deep Anomaly Detection with Scale Learning\n",
    "| DevNet      | KDD   | 2019 | Weakly-Supervised | Deep Anomaly Detection with Deviation Networks\n",
    "| PReNet      | KDD   | 2023 | Weakly-Supervised | Deep Weakly-supervised Anomaly Detection\n",
    "| Deep SAD    | ICLR  | 2020 | Weakly-Supervised | Deep Semi-Supervised Anomaly Detection\n",
    "| FeaWAD      | TNNLS | 2021 | Weakly-Supervised | Feature Encoding with AutoEncoders for Weakly-supervised Anomaly Detection\n",
    "\n",
    "**Usage Examples:**\n",
    "\n",
    "DeepOD is user-friendly and can be employed with only a few lines of code. The API style is similar to sklearn and [PyOD](https://github.com/yzhao062/pyod).\n",
    "\n",
    "```python\n",
    "# Unsupservised methods\n",
    "from deepod.models.dsvdd import DeepSVDD\n",
    "clf = DeepSVDD()\n",
    "clf.fit(X_train, y=None)\n",
    "scores = clf.decision_function(X_test)\n",
    "\n",
    "# Weakly-supervised methods\n",
    "from deepod.models.devnet import DevNet\n",
    "clf = DevNet()\n",
    "clf.fit(X_train, y=semi_y)  # semi_y uses 1 for known anomalies and 0 for unlabeled data\n",
    "scores = clf.decision_function(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a0ffb4",
   "metadata": {},
   "source": [
    "## ICL: Anomaly Detection for Tabular Data with Internal Contrastive Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2aef1a3",
   "metadata": {},
   "source": [
    "The **ICL method** focuses on **out-of-class sample detection** in **tabular data**, where the data structure is largely unknown. It aims to **capture the structure** of single training class samples by learning mappings that **maximize the mutual information** between each sample and the **masked-out portion**. This is accomplished through a **contrastive loss**, learning mappings individually for each sample. The score for a test sample is determined by measuring the **contrastive loss of the learned mappings applied to the masked parts** of the sample. ICL **outperforms existing methods** with a substantial accuracy advantage across benchmarks.\n",
    "\n",
    "\n",
    "**Contrastive learning**, rooted in metric learning and unsupervised representation learning, maximizes similarity between queries and positive samples while minimizing it with negative samples. Existing methods, often using image transformations, **don't apply to tabular data** due to the absence of invariant transformation groups. Qiu et al.'s NeuTraL AD work is a similar method for tabular data but **differs significantly**, utilizing specific masks and single feature extractors.\n",
    "\n",
    "\n",
    "ICL employs a training set of in-class samples and aims to design a **scoring function** that maps samples to low values if they belong to the same distribution and high values otherwise. Hyperparameters include **k for feature subset size, u for embedding size, and τ for loss temperature**. The method **constructs pairs from each sample and learns mappings to maximize mutual information**. Mutual information maximization is achieved through the **noise contrastive estimation framework**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d977a45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepod.models.icl import ICL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "74afc862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unsupervised method\n",
    "clf = ICL(epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "4cf132b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training...\n",
      "ensemble size: 2\n",
      "kernel size: 10\n",
      "ICLNet(\n",
      "  (enc_f_net): MLPnet(\n",
      "    (network): Sequential(\n",
      "      (0): LinearBlock(\n",
      "        (linear): Linear(in_features=40, out_features=100, bias=False)\n",
      "        (act_layer): Tanh()\n",
      "        (bn_layer): BatchNorm1d(41, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      )\n",
      "      (1): LinearBlock(\n",
      "        (linear): Linear(in_features=100, out_features=50, bias=False)\n",
      "        (act_layer): LeakyReLU(negative_slope=0.01)\n",
      "        (bn_layer): BatchNorm1d(41, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      )\n",
      "      (2): LinearBlock(\n",
      "        (linear): Linear(in_features=50, out_features=128, bias=False)\n",
      "        (act_layer): LeakyReLU(negative_slope=0.01)\n",
      "        (bn_layer): BatchNorm1d(41, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (enc_g_net): MLPnet(\n",
      "    (network): Sequential(\n",
      "      (0): LinearBlock(\n",
      "        (linear): Linear(in_features=10, out_features=50, bias=False)\n",
      "        (act_layer): LeakyReLU(negative_slope=0.01)\n",
      "        (bn_layer): BatchNorm1d(41, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      )\n",
      "      (1): LinearBlock(\n",
      "        (linear): Linear(in_features=50, out_features=25, bias=False)\n",
      "        (act_layer): LeakyReLU(negative_slope=0.01)\n",
      "        (bn_layer): BatchNorm1d(41, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      )\n",
      "      (2): LinearBlock(\n",
      "        (linear): Linear(in_features=25, out_features=128, bias=False)\n",
      "        (act_layer): LeakyReLU(negative_slope=0.01)\n",
      "        (bn_layer): BatchNorm1d(41, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "epoch  1, training loss: 2.215789, time: 103.8s\n",
      "kernel size: 10\n",
      "ICLNet(\n",
      "  (enc_f_net): MLPnet(\n",
      "    (network): Sequential(\n",
      "      (0): LinearBlock(\n",
      "        (linear): Linear(in_features=40, out_features=100, bias=False)\n",
      "        (act_layer): Tanh()\n",
      "        (bn_layer): BatchNorm1d(41, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      )\n",
      "      (1): LinearBlock(\n",
      "        (linear): Linear(in_features=100, out_features=50, bias=False)\n",
      "        (act_layer): LeakyReLU(negative_slope=0.01)\n",
      "        (bn_layer): BatchNorm1d(41, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      )\n",
      "      (2): LinearBlock(\n",
      "        (linear): Linear(in_features=50, out_features=128, bias=False)\n",
      "        (act_layer): LeakyReLU(negative_slope=0.01)\n",
      "        (bn_layer): BatchNorm1d(41, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (enc_g_net): MLPnet(\n",
      "    (network): Sequential(\n",
      "      (0): LinearBlock(\n",
      "        (linear): Linear(in_features=10, out_features=50, bias=False)\n",
      "        (act_layer): LeakyReLU(negative_slope=0.01)\n",
      "        (bn_layer): BatchNorm1d(41, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      )\n",
      "      (1): LinearBlock(\n",
      "        (linear): Linear(in_features=50, out_features=25, bias=False)\n",
      "        (act_layer): LeakyReLU(negative_slope=0.01)\n",
      "        (bn_layer): BatchNorm1d(41, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      )\n",
      "      (2): LinearBlock(\n",
      "        (linear): Linear(in_features=25, out_features=128, bias=False)\n",
      "        (act_layer): LeakyReLU(negative_slope=0.01)\n",
      "        (bn_layer): BatchNorm1d(41, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "epoch  1, training loss: 2.135577, time: 105.0s\n",
      "Start Inference on the training data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "testing: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 12422/12422 [00:41<00:00, 299.47it/s]\n",
      "testing: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 12422/12422 [00:40<00:00, 304.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Time: 1137.8853588104248 seconds\n"
     ]
    }
   ],
   "source": [
    "# Start measuring training time\n",
    "start_time = time.time()\n",
    "\n",
    "# Train ICL\n",
    "clf.fit(X_train.values, y=None)\n",
    "\n",
    "# End measuring training time\n",
    "end_time = time.time()\n",
    "\n",
    "# Storing and printing the time\n",
    "icl_training_time = end_time - start_time\n",
    "print(f\"Training Time: {icl_training_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "e305b153",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "testing: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3204/3204 [00:10<00:00, 306.23it/s]\n",
      "testing: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3204/3204 [00:10<00:00, 308.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference Time: 21.296762228012085 seconds\n"
     ]
    }
   ],
   "source": [
    "# Start measuring inference time\n",
    "start_time = time.time()\n",
    "\n",
    "# Predict on the test set\n",
    "icl_scored_test = clf.decision_function(X_test.values)\n",
    "\n",
    "# End measuring inference time\n",
    "end_time = time.time()\n",
    "\n",
    "# Storing and printing the time\n",
    "icl_inference_time = end_time - start_time\n",
    "print(f\"Inference Time: {icl_inference_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5544169",
   "metadata": {},
   "source": [
    "If you want to save the model, you can use:\n",
    "\n",
    "```python\n",
    "torch.save(clf.net.state_dict(), \"PATH_TO_SAVE_MODEL\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "2c8bff50",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_times_dict[(variant.split('.')[0], \"ICL\")] = {\"Training Time\":icl_training_time}\n",
    "inference_times_dict[(variant.split('.')[0], \"ICL\")] = {\"Inference Time\":icl_inference_time}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "b02dbc2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model TPR: 0.0865, Model FPR: 0.0499, Model Threshold: 3.1268, Model AUROC: 0.6172\n"
     ]
    }
   ],
   "source": [
    "# Calculate the TPR@5%FPR on the test set\n",
    "metrics_dict = calculate_tpr_at_fpr(y_test, icl_scored_test, fpr_lim=0.05)\n",
    "performance_results[(variant.split('.')[0], 'ICL')] = metrics_dict\n",
    "print(f\"Model TPR: {metrics_dict['TPR']}, Model FPR: {metrics_dict['FPR']}, Model Threshold: {metrics_dict['Threshold']}, Model AUROC: {metrics_dict['AUROC']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "5352bc16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fairness Ratio: 0.9953 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate the fairness metrics on the test set\n",
    "fairness_ratio = calculate_fairness_metrics(y_test, icl_scored_test, X_test, fpr_lim=0.05)\n",
    "fairness_results[(variant.split('.')[0], 'ICL')] = {\"Fairness Ratio\":fairness_ratio}\n",
    "print(f\"Fairness Ratio: {fairness_ratio}\", \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "b2f4ad85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "performance_results_path = os.path.join(model_path, 'performance_results.pkl')\n",
    "fairness_results_results_path = os.path.join(model_path, 'fairness_results.pkl')\n",
    "training_times_dict_path = os.path.join(model_path, 'training_times_dict.pkl')\n",
    "inference_times_dict_path = os.path.join(model_path, 'inference_times_dict.pkl')\n",
    "\n",
    "with open(performance_results_path, 'wb') as file:\n",
    "    pickle.dump(performance_results, file)\n",
    "    \n",
    "with open(fairness_results_results_path, 'wb') as file:\n",
    "    pickle.dump(fairness_results, file)\n",
    "    \n",
    "with open(training_times_dict_path, 'wb') as file:\n",
    "    pickle.dump(training_times_dict, file)\n",
    "    \n",
    "with open(inference_times_dict_path, 'wb') as file:\n",
    "    pickle.dump(inference_times_dict, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf3796e",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "In the forthcoming section, we will present the results obtained from our comprehensive analysis. We will showcase the performance metrics of the evaluated models on the BAF dataset, delving into their effectiveness in detecting fraudulent online bank account opening applications. Moreover, we will explore the fairness metrics, ensuring unbiased decision-making in the context of different user groups. To assess the real-world applicability of the models, we will provide insights into their training and inference times, offering valuable insights into their efficiency and suitability for practical use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2458ac75",
   "metadata": {},
   "source": [
    "## Table\n",
    "\n",
    "Below you can see all the result in one table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "08effeef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame from all results\n",
    "df_performance_results = pd.DataFrame(performance_results).transpose()\n",
    "df_fairness_results = pd.DataFrame(fairness_results).transpose()\n",
    "df_training_times_dict = pd.DataFrame(training_times_dict).transpose()\n",
    "df_inference_times_dict = pd.DataFrame(inference_times_dict).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "1c5ad1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate all results together\n",
    "all_results = pd.concat([df_performance_results, df_fairness_results, df_training_times_dict, df_inference_times_dict], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "96344431",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>TPR</th>\n",
       "      <th>FPR</th>\n",
       "      <th>Threshold</th>\n",
       "      <th>AUROC</th>\n",
       "      <th>Fairness Ratio</th>\n",
       "      <th>Training Time</th>\n",
       "      <th>Inference Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"9\" valign=\"top\">Base</th>\n",
       "      <th>Logistic Regression</th>\n",
       "      <td>0.2074</td>\n",
       "      <td>0.0497</td>\n",
       "      <td>0.112200</td>\n",
       "      <td>0.6802</td>\n",
       "      <td>0.3455</td>\n",
       "      <td>18.675150</td>\n",
       "      <td>0.080438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.4288</td>\n",
       "      <td>0.0415</td>\n",
       "      <td>0.090000</td>\n",
       "      <td>0.8398</td>\n",
       "      <td>0.3007</td>\n",
       "      <td>283.078872</td>\n",
       "      <td>3.608654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBoost</th>\n",
       "      <td>0.5049</td>\n",
       "      <td>0.0498</td>\n",
       "      <td>0.041100</td>\n",
       "      <td>0.8787</td>\n",
       "      <td>0.3233</td>\n",
       "      <td>307.644614</td>\n",
       "      <td>0.549125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CatBoost</th>\n",
       "      <td>0.5101</td>\n",
       "      <td>0.0499</td>\n",
       "      <td>0.037300</td>\n",
       "      <td>0.8836</td>\n",
       "      <td>0.2992</td>\n",
       "      <td>198.038225</td>\n",
       "      <td>0.373308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Light GBM</th>\n",
       "      <td>0.5302</td>\n",
       "      <td>0.0500</td>\n",
       "      <td>0.040400</td>\n",
       "      <td>0.8898</td>\n",
       "      <td>0.3017</td>\n",
       "      <td>13.898896</td>\n",
       "      <td>1.010416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TabNet</th>\n",
       "      <td>0.5069</td>\n",
       "      <td>0.0500</td>\n",
       "      <td>0.028200</td>\n",
       "      <td>0.8822</td>\n",
       "      <td>0.3459</td>\n",
       "      <td>666.011628</td>\n",
       "      <td>6.612230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AE</th>\n",
       "      <td>0.0629</td>\n",
       "      <td>0.0498</td>\n",
       "      <td>1837.917969</td>\n",
       "      <td>0.6143</td>\n",
       "      <td>0.3692</td>\n",
       "      <td>50.658261</td>\n",
       "      <td>0.110126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IF</th>\n",
       "      <td>0.0806</td>\n",
       "      <td>0.0499</td>\n",
       "      <td>0.015500</td>\n",
       "      <td>0.5816</td>\n",
       "      <td>0.2516</td>\n",
       "      <td>0.915280</td>\n",
       "      <td>1.485850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ICL</th>\n",
       "      <td>0.0865</td>\n",
       "      <td>0.0499</td>\n",
       "      <td>3.126800</td>\n",
       "      <td>0.6172</td>\n",
       "      <td>0.9953</td>\n",
       "      <td>1137.885359</td>\n",
       "      <td>21.296762</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             TPR     FPR    Threshold   AUROC  Fairness Ratio  \\\n",
       "Base Logistic Regression  0.2074  0.0497     0.112200  0.6802          0.3455   \n",
       "     Random Forest        0.4288  0.0415     0.090000  0.8398          0.3007   \n",
       "     XGBoost              0.5049  0.0498     0.041100  0.8787          0.3233   \n",
       "     CatBoost             0.5101  0.0499     0.037300  0.8836          0.2992   \n",
       "     Light GBM            0.5302  0.0500     0.040400  0.8898          0.3017   \n",
       "     TabNet               0.5069  0.0500     0.028200  0.8822          0.3459   \n",
       "     AE                   0.0629  0.0498  1837.917969  0.6143          0.3692   \n",
       "     IF                   0.0806  0.0499     0.015500  0.5816          0.2516   \n",
       "     ICL                  0.0865  0.0499     3.126800  0.6172          0.9953   \n",
       "\n",
       "                          Training Time  Inference Time  \n",
       "Base Logistic Regression      18.675150        0.080438  \n",
       "     Random Forest           283.078872        3.608654  \n",
       "     XGBoost                 307.644614        0.549125  \n",
       "     CatBoost                198.038225        0.373308  \n",
       "     Light GBM                13.898896        1.010416  \n",
       "     TabNet                  666.011628        6.612230  \n",
       "     AE                       50.658261        0.110126  \n",
       "     IF                        0.915280        1.485850  \n",
       "     ICL                    1137.885359       21.296762  "
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258e057a",
   "metadata": {},
   "source": [
    "## Plots\n",
    "\n",
    "The following is 4 plots demonstrating all results for all models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "27fdb0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract TPR values\n",
    "lr_tpr = df_performance_results.loc[(variant.split('.')[0], 'Logistic Regression'), 'TPR']\n",
    "rf_tpr = df_performance_results.loc[(variant.split('.')[0], 'Random Forest'), 'TPR']\n",
    "xgb_tpr = df_performance_results.loc[(variant.split('.')[0], 'XGBoost'), 'TPR']\n",
    "cb_tpr = df_performance_results.loc[(variant.split('.')[0], 'CatBoost'), 'TPR']\n",
    "lgbm_tpr = df_performance_results.loc[(variant.split('.')[0], 'Light GBM'), 'TPR']\n",
    "tabnet_tpr = df_performance_results.loc[(variant.split('.')[0], 'TabNet'), 'TPR']\n",
    "ae_tpr = df_performance_results.loc[(variant.split('.')[0], 'AE'), 'TPR']\n",
    "if_tpr = df_performance_results.loc[(variant.split('.')[0], 'IF'), 'TPR']\n",
    "icl_tpr = df_performance_results.loc[(variant.split('.')[0], 'ICL'), 'TPR']\n",
    "tpr_values = [lr_tpr, rf_tpr, xgb_tpr, cb_tpr, lgbm_tpr, tabnet_tpr, ae_tpr, if_tpr, icl_tpr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "e3d13940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Fairness Ratio values\n",
    "lr_fn = df_fairness_results.loc[(variant.split('.')[0], 'Logistic Regression'), 'Fairness Ratio']\n",
    "rf_fn = df_fairness_results.loc[(variant.split('.')[0], 'Random Forest'), 'Fairness Ratio']\n",
    "xgb_fn = df_fairness_results.loc[(variant.split('.')[0], 'XGBoost'), 'Fairness Ratio']\n",
    "cb_fn = df_fairness_results.loc[(variant.split('.')[0], 'CatBoost'), 'Fairness Ratio']\n",
    "lgbm_fn = df_fairness_results.loc[(variant.split('.')[0], 'Light GBM'), 'Fairness Ratio']\n",
    "tabnet_fn = df_fairness_results.loc[(variant.split('.')[0], 'TabNet'), 'Fairness Ratio']\n",
    "ae_fn = df_fairness_results.loc[(variant.split('.')[0], 'AE'), 'Fairness Ratio']\n",
    "if_fn = df_fairness_results.loc[(variant.split('.')[0], 'IF'), 'Fairness Ratio']\n",
    "icl_fn = df_fairness_results.loc[(variant.split('.')[0], 'ICL'), 'Fairness Ratio']\n",
    "fn_values = [lr_fn, rf_fn, xgb_fn, cb_fn, lgbm_fn, tabnet_fn, ae_fn, if_fn, icl_fn]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "1a79ed11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Training Time values\n",
    "lr_tnt = df_training_times_dict.loc[(variant.split('.')[0], 'Logistic Regression'), 'Training Time']\n",
    "rf_tnt = df_training_times_dict.loc[(variant.split('.')[0], 'Random Forest'), 'Training Time']\n",
    "xgb_tnt = df_training_times_dict.loc[(variant.split('.')[0], 'XGBoost'), 'Training Time']\n",
    "cb_tnt = df_training_times_dict.loc[(variant.split('.')[0], 'CatBoost'), 'Training Time']\n",
    "lgbm_tnt = df_training_times_dict.loc[(variant.split('.')[0], 'Light GBM'), 'Training Time']\n",
    "tabnet_tnt = df_training_times_dict.loc[(variant.split('.')[0], 'TabNet'), 'Training Time']\n",
    "ae_tnt = df_training_times_dict.loc[(variant.split('.')[0], 'AE'), 'Training Time']\n",
    "if_tnt = df_training_times_dict.loc[(variant.split('.')[0], 'IF'), 'Training Time']\n",
    "icl_tnt = df_training_times_dict.loc[(variant.split('.')[0], 'ICL'), 'Training Time']\n",
    "tnt_values = [lr_tnt, rf_tnt, xgb_tnt, cb_tnt, lgbm_tnt, tabnet_tnt, ae_tnt, if_tnt, icl_tnt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "29e6f720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Inference Time values\n",
    "lr_inf = df_inference_times_dict.loc[(variant.split('.')[0], 'Logistic Regression'), 'Inference Time']\n",
    "rf_inf = df_inference_times_dict.loc[(variant.split('.')[0], 'Random Forest'), 'Inference Time']\n",
    "xgb_inf = df_inference_times_dict.loc[(variant.split('.')[0], 'XGBoost'), 'Inference Time']\n",
    "cb_inf = df_inference_times_dict.loc[(variant.split('.')[0], 'CatBoost'), 'Inference Time']\n",
    "lgbm_inf = df_inference_times_dict.loc[(variant.split('.')[0], 'Light GBM'), 'Inference Time']\n",
    "tabnet_inf = df_inference_times_dict.loc[(variant.split('.')[0], 'TabNet'), 'Inference Time']\n",
    "ae_inf = df_inference_times_dict.loc[(variant.split('.')[0], 'AE'), 'Inference Time']\n",
    "if_inf = df_inference_times_dict.loc[(variant.split('.')[0], 'IF'), 'Inference Time']\n",
    "icl_inf = df_inference_times_dict.loc[(variant.split('.')[0], 'ICL'), 'Inference Time']\n",
    "inf_values = [lr_inf, rf_inf, xgb_inf, cb_inf, lgbm_inf, tabnet_inf, ae_inf, if_inf, icl_inf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "9f69d9d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x1000 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a 2x2 grid of subplots\n",
    "fig, axs = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "models = ['LR', 'RF', 'XGB', 'CB', 'LGBM', 'TabNet', 'AE', 'IF', 'ICL']\n",
    "\n",
    "# Plot TPR values\n",
    "axs[0, 0].bar(models, tpr_values)\n",
    "axs[0, 0].set_xlabel('Models')\n",
    "axs[0, 0].set_ylabel('TPR@5%FPR')\n",
    "axs[0, 0].set_title('Performance')\n",
    "\n",
    "# Plot Fairness Ratio values\n",
    "axs[0, 1].bar(models, fn_values)\n",
    "axs[0, 1].set_xlabel('Models')\n",
    "axs[0, 1].set_ylabel('Fairness Ratio')\n",
    "axs[0, 1].set_title('Fairness')\n",
    "\n",
    "# Plot Training Time values\n",
    "axs[1, 0].bar(models, tnt_values)\n",
    "axs[1, 0].set_xlabel('Models')\n",
    "axs[1, 0].set_ylabel('Training Time')\n",
    "axs[1, 0].set_title('Time')\n",
    "\n",
    "# Plot Inference Time values\n",
    "axs[1, 1].bar(models, inf_values)\n",
    "axs[1, 1].set_xlabel('Models')\n",
    "axs[1, 1].set_ylabel('Inference Time')\n",
    "axs[1, 1].set_title('Time')\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the combined plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "5d1ad3b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the list of model names and their corresponding y_scores\n",
    "model_names = ['LR', 'RF', 'XGB', 'CB', 'LGBM', 'TabNet', 'AE', 'IF', 'ICL']\n",
    "y_scores_dict = {\n",
    "    'LR': lr_scored_test,\n",
    "    'RF': rf_scored_test,\n",
    "    'XGB': xgb_scored_test,\n",
    "    'CB': cb_scored_test,\n",
    "    'LGBM': lgbm_scored_test,\n",
    "    'TabNet': tabnet_scored_test,\n",
    "    'AE': reconstruction_errors.numpy(),\n",
    "    'IF': -isolation_scores,\n",
    "    'ICL': icl_scored_test,\n",
    "}\n",
    "\n",
    "# Generate FPR values from 5% to 40%\n",
    "fpr_values = np.logspace(np.log10(0.05), np.log10(0.4), num=100)\n",
    "\n",
    "# Initialize a dictionary to store interpolated TPR values for each model\n",
    "interpolated_tpr = {}\n",
    "\n",
    "# Calculate TPR values for each model at the desired FPR points\n",
    "for model_name in model_names:\n",
    "    scored_test = y_scores_dict[model_name]\n",
    "    tpr_list = [calculate_tpr_at_fpr(y_test, scored_test, fpr_lim=i)['TPR'] for i in fpr_values]\n",
    "    interpolated_tpr[model_name] = tpr_list\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "for model_name, model_tpr_interp in interpolated_tpr.items():\n",
    "    plt.plot(fpr_values, model_tpr_interp, label=model_name)\n",
    "\n",
    "plt.xscale('log')\n",
    "plt.xlabel('False Positive Rate (FPR)')\n",
    "plt.ylabel('True Positive Rate (TPR)')\n",
    "plt.title('TPR vs. FPR for Different Models')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa77ba0",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning\n",
    "\n",
    "Hyperparameter tuning is the process of finding the best set of hyperparameters for a machine learning model to achieve optimal performance. Hyperparameters are configuration settings for a model that cannot be learned directly from the data and need to be set before training the model. They control various aspects of the learning process and can significantly impact the model's performance.\n",
    "\n",
    "The provided code performs hyperparameter tuning for LightGBM on BAF dataset using the GridSearchCV method from scikit-learn. \n",
    "\n",
    "The code defines a custom metric function 'TPR_at_05FPR', which calculates the True Positive Rate (TPR) when the False Positive Rate (FPR) is at 5%. This custom metric will be used during the hyperparameter tuning process to optimize for TPR@5%FPR.\n",
    "\n",
    "Hyperparameter grids for each model are defined in the 'param_grids' dictionary. Each model has different hyperparameters, and their possible values are specified in the dictionary. The hyperparameter tuning is performed within a loop that iterates over each model in the 'models' dictionary. For each model, GridSearchCV is used to find the best combination of hyperparameters from the corresponding 'param_grids' using the custom metric 'TPR_at_05FPR' as the scoring metric. The hyperparameter tuning is performed using 5-fold cross-validation.\n",
    "\n",
    "After finding the best hyperparameters, the code trains the model on the training data using these optimized hyperparameters. Then, it predicts the target variable on the test data and evaluates the model's performance, which computes various metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22f56afc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameter tuning for LightGBM on Base...\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Info] Number of positive: 6520, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015367 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010252 -> initscore=-4.570005\n",
      "[LightGBM] [Info] Start training from score -4.570005\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018293 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017370 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019487 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018744 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635992, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569852\n",
      "[LightGBM] [Info] Start training from score -4.569852\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Info] Number of positive: 6520, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018438 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010252 -> initscore=-4.570005\n",
      "[LightGBM] [Info] Start training from score -4.570005\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015366 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018463 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017841 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.020112 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635992, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569852\n",
      "[LightGBM] [Info] Start training from score -4.569852\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Info] Number of positive: 6520, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018629 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010252 -> initscore=-4.570005\n",
      "[LightGBM] [Info] Start training from score -4.570005\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014913 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016276 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.020413 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015806 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635992, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569852\n",
      "[LightGBM] [Info] Start training from score -4.569852\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Info] Number of positive: 6520, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015296 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010252 -> initscore=-4.570005\n",
      "[LightGBM] [Info] Start training from score -4.570005\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.020347 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019270 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016766 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017777 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635992, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569852\n",
      "[LightGBM] [Info] Start training from score -4.569852\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Info] Number of positive: 6520, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015348 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010252 -> initscore=-4.570005\n",
      "[LightGBM] [Info] Start training from score -4.570005\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017816 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017432 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017675 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018346 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635992, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569852\n",
      "[LightGBM] [Info] Start training from score -4.569852\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Info] Number of positive: 6520, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015249 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010252 -> initscore=-4.570005\n",
      "[LightGBM] [Info] Start training from score -4.570005\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015802 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018220 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018814 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015980 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635992, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569852\n",
      "[LightGBM] [Info] Start training from score -4.569852\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Info] Number of positive: 6520, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017288 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010252 -> initscore=-4.570005\n",
      "[LightGBM] [Info] Start training from score -4.570005\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017677 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018878 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016295 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016859 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635992, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569852\n",
      "[LightGBM] [Info] Start training from score -4.569852\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Info] Number of positive: 6520, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018385 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010252 -> initscore=-4.570005\n",
      "[LightGBM] [Info] Start training from score -4.570005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.021837 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016609 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015981 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014988 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635992, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569852\n",
      "[LightGBM] [Info] Start training from score -4.569852\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Info] Number of positive: 6520, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015736 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010252 -> initscore=-4.570005\n",
      "[LightGBM] [Info] Start training from score -4.570005\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015750 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016402 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.020484 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019222 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635992, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569852\n",
      "[LightGBM] [Info] Start training from score -4.569852\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Info] Number of positive: 6520, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018783 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010252 -> initscore=-4.570005\n",
      "[LightGBM] [Info] Start training from score -4.570005\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.020499 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016736 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018805 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015758 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635992, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569852\n",
      "[LightGBM] [Info] Start training from score -4.569852\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Info] Number of positive: 6520, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018763 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010252 -> initscore=-4.570005\n",
      "[LightGBM] [Info] Start training from score -4.570005\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017761 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016087 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019762 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016659 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635992, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569852\n",
      "[LightGBM] [Info] Start training from score -4.569852\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Info] Number of positive: 6520, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018772 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010252 -> initscore=-4.570005\n",
      "[LightGBM] [Info] Start training from score -4.570005\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016071 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017805 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019718 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018970 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635992, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569852\n",
      "[LightGBM] [Info] Start training from score -4.569852\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Info] Number of positive: 6520, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016665 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010252 -> initscore=-4.570005\n",
      "[LightGBM] [Info] Start training from score -4.570005\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013691 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016086 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019961 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017118 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635992, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569852\n",
      "[LightGBM] [Info] Start training from score -4.569852\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Info] Number of positive: 6520, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015077 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010252 -> initscore=-4.570005\n",
      "[LightGBM] [Info] Start training from score -4.570005\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014118 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.020849 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019824 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019503 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635992, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569852\n",
      "[LightGBM] [Info] Start training from score -4.569852\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Info] Number of positive: 6520, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015595 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010252 -> initscore=-4.570005\n",
      "[LightGBM] [Info] Start training from score -4.570005\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015789 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015617 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019811 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015880 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635992, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569852\n",
      "[LightGBM] [Info] Start training from score -4.569852\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Info] Number of positive: 6520, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015891 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010252 -> initscore=-4.570005\n",
      "[LightGBM] [Info] Start training from score -4.570005\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016012 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017403 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.020723 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015820 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635992, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569852\n",
      "[LightGBM] [Info] Start training from score -4.569852\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Info] Number of positive: 6520, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015825 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010252 -> initscore=-4.570005\n",
      "[LightGBM] [Info] Start training from score -4.570005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015920 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018334 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.020551 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016878 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635992, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569852\n",
      "[LightGBM] [Info] Start training from score -4.569852\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Info] Number of positive: 6520, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013855 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010252 -> initscore=-4.570005\n",
      "[LightGBM] [Info] Start training from score -4.570005\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016130 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015498 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017082 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015799 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635992, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569852\n",
      "[LightGBM] [Info] Start training from score -4.569852\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 6520, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019519 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010252 -> initscore=-4.570005\n",
      "[LightGBM] [Info] Start training from score -4.570005\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016756 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015215 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017704 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015890 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635992, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569852\n",
      "[LightGBM] [Info] Start training from score -4.569852\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 6520, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017898 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010252 -> initscore=-4.570005\n",
      "[LightGBM] [Info] Start training from score -4.570005\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018504 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014920 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015943 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015638 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635992, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569852\n",
      "[LightGBM] [Info] Start training from score -4.569852\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 6520, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014710 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010252 -> initscore=-4.570005\n",
      "[LightGBM] [Info] Start training from score -4.570005\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016427 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014845 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019018 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015988 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635992, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569852\n",
      "[LightGBM] [Info] Start training from score -4.569852\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 6520, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016744 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010252 -> initscore=-4.570005\n",
      "[LightGBM] [Info] Start training from score -4.570005\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014444 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016047 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.020037 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015950 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635992, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569852\n",
      "[LightGBM] [Info] Start training from score -4.569852\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 6520, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019053 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010252 -> initscore=-4.570005\n",
      "[LightGBM] [Info] Start training from score -4.570005\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016519 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018594 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015771 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018824 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635992, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569852\n",
      "[LightGBM] [Info] Start training from score -4.569852\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 6520, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018397 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010252 -> initscore=-4.570005\n",
      "[LightGBM] [Info] Start training from score -4.570005\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015773 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017644 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.021609 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019622 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635992, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569852\n",
      "[LightGBM] [Info] Start training from score -4.569852\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 6520, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016342 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010252 -> initscore=-4.570005\n",
      "[LightGBM] [Info] Start training from score -4.570005\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014452 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018783 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017618 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017796 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635992, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569852\n",
      "[LightGBM] [Info] Start training from score -4.569852\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 6520, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017986 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010252 -> initscore=-4.570005\n",
      "[LightGBM] [Info] Start training from score -4.570005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016179 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015113 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017542 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016367 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635992, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569852\n",
      "[LightGBM] [Info] Start training from score -4.569852\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 6520, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.020808 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010252 -> initscore=-4.570005\n",
      "[LightGBM] [Info] Start training from score -4.570005\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016003 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014831 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.020081 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016169 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635992, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569852\n",
      "[LightGBM] [Info] Start training from score -4.569852\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Info] Number of positive: 6520, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018906 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010252 -> initscore=-4.570005\n",
      "[LightGBM] [Info] Start training from score -4.570005\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019670 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017934 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019597 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019087 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635992, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569852\n",
      "[LightGBM] [Info] Start training from score -4.569852\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Info] Number of positive: 6520, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014143 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010252 -> initscore=-4.570005\n",
      "[LightGBM] [Info] Start training from score -4.570005\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014172 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015531 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019765 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015569 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635992, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569852\n",
      "[LightGBM] [Info] Start training from score -4.569852\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Info] Number of positive: 6520, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016058 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010252 -> initscore=-4.570005\n",
      "[LightGBM] [Info] Start training from score -4.570005\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017693 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.021352 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016509 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015096 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635992, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569852\n",
      "[LightGBM] [Info] Start training from score -4.569852\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Info] Number of positive: 6520, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017804 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010252 -> initscore=-4.570005\n",
      "[LightGBM] [Info] Start training from score -4.570005\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017668 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013963 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019799 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014411 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635992, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569852\n",
      "[LightGBM] [Info] Start training from score -4.569852\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Info] Number of positive: 6520, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018045 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010252 -> initscore=-4.570005\n",
      "[LightGBM] [Info] Start training from score -4.570005\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018999 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015257 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019966 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015443 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635992, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569852\n",
      "[LightGBM] [Info] Start training from score -4.569852\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Info] Number of positive: 6520, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015741 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010252 -> initscore=-4.570005\n",
      "[LightGBM] [Info] Start training from score -4.570005\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015517 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015761 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019647 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016162 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635992, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569852\n",
      "[LightGBM] [Info] Start training from score -4.569852\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Info] Number of positive: 6520, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017501 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010252 -> initscore=-4.570005\n",
      "[LightGBM] [Info] Start training from score -4.570005\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015534 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015607 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019932 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017044 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635992, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569852\n",
      "[LightGBM] [Info] Start training from score -4.569852\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Info] Number of positive: 6520, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018274 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010252 -> initscore=-4.570005\n",
      "[LightGBM] [Info] Start training from score -4.570005\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018316 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016019 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016008 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015320 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635992, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569852\n",
      "[LightGBM] [Info] Start training from score -4.569852\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Info] Number of positive: 6520, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016038 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010252 -> initscore=-4.570005\n",
      "[LightGBM] [Info] Start training from score -4.570005\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015189 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015340 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019993 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015893 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635992, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569852\n",
      "[LightGBM] [Info] Start training from score -4.569852\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Info] Number of positive: 6520, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.020299 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010252 -> initscore=-4.570005\n",
      "[LightGBM] [Info] Start training from score -4.570005\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018844 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016314 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019191 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015168 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635992, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569852\n",
      "[LightGBM] [Info] Start training from score -4.569852\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Info] Number of positive: 6520, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016764 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010252 -> initscore=-4.570005\n",
      "[LightGBM] [Info] Start training from score -4.570005\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018462 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015676 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019694 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016508 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635992, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569852\n",
      "[LightGBM] [Info] Start training from score -4.569852\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Info] Number of positive: 6520, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019333 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010252 -> initscore=-4.570005\n",
      "[LightGBM] [Info] Start training from score -4.570005\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015807 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018483 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019819 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016190 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635992, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569852\n",
      "[LightGBM] [Info] Start training from score -4.569852\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Info] Number of positive: 6520, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016382 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010252 -> initscore=-4.570005\n",
      "[LightGBM] [Info] Start training from score -4.570005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015885 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016258 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019200 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.021197 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635992, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569852\n",
      "[LightGBM] [Info] Start training from score -4.569852\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Info] Number of positive: 6520, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014619 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010252 -> initscore=-4.570005\n",
      "[LightGBM] [Info] Start training from score -4.570005\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.021124 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015589 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017746 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019852 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635992, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569852\n",
      "[LightGBM] [Info] Start training from score -4.569852\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Info] Number of positive: 6520, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.020240 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010252 -> initscore=-4.570005\n",
      "[LightGBM] [Info] Start training from score -4.570005\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017963 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016208 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019875 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018810 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635992, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569852\n",
      "[LightGBM] [Info] Start training from score -4.569852\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Info] Number of positive: 6520, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017689 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010252 -> initscore=-4.570005\n",
      "[LightGBM] [Info] Start training from score -4.570005\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018503 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016455 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.020933 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016526 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635992, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569852\n",
      "[LightGBM] [Info] Start training from score -4.569852\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Info] Number of positive: 6520, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015636 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010252 -> initscore=-4.570005\n",
      "[LightGBM] [Info] Start training from score -4.570005\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.020185 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016480 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017485 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019614 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635992, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569852\n",
      "[LightGBM] [Info] Start training from score -4.569852\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Info] Number of positive: 6520, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016346 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010252 -> initscore=-4.570005\n",
      "[LightGBM] [Info] Start training from score -4.570005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018408 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015158 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.020302 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018758 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635992, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569852\n",
      "[LightGBM] [Info] Start training from score -4.569852\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 6520, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018973 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010252 -> initscore=-4.570005\n",
      "[LightGBM] [Info] Start training from score -4.570005\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016525 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015752 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016767 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018415 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635992, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569852\n",
      "[LightGBM] [Info] Start training from score -4.569852\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 6520, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016016 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010252 -> initscore=-4.570005\n",
      "[LightGBM] [Info] Start training from score -4.570005\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018947 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016543 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019891 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015905 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635992, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569852\n",
      "[LightGBM] [Info] Start training from score -4.569852\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 6520, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016615 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010252 -> initscore=-4.570005\n",
      "[LightGBM] [Info] Start training from score -4.570005\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018273 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018890 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018151 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016020 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635992, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569852\n",
      "[LightGBM] [Info] Start training from score -4.569852\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 6520, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016939 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010252 -> initscore=-4.570005\n",
      "[LightGBM] [Info] Start training from score -4.570005\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015218 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015819 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016556 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015940 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635992, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569852\n",
      "[LightGBM] [Info] Start training from score -4.569852\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 6520, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017882 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010252 -> initscore=-4.570005\n",
      "[LightGBM] [Info] Start training from score -4.570005\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016676 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015336 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019904 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016396 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635992, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569852\n",
      "[LightGBM] [Info] Start training from score -4.569852\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 6520, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017993 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010252 -> initscore=-4.570005\n",
      "[LightGBM] [Info] Start training from score -4.570005\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019345 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016125 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017019 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016686 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635992, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569852\n",
      "[LightGBM] [Info] Start training from score -4.569852\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 6520, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015424 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010252 -> initscore=-4.570005\n",
      "[LightGBM] [Info] Start training from score -4.570005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016094 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015793 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017201 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015842 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635992, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569852\n",
      "[LightGBM] [Info] Start training from score -4.569852\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 6520, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015132 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010252 -> initscore=-4.570005\n",
      "[LightGBM] [Info] Start training from score -4.570005\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016577 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016352 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019718 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015197 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635992, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569852\n",
      "[LightGBM] [Info] Start training from score -4.569852\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 6520, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016043 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010252 -> initscore=-4.570005\n",
      "[LightGBM] [Info] Start training from score -4.570005\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014191 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018542 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019087 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016744 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635992, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569852\n",
      "[LightGBM] [Info] Start training from score -4.569852\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Info] Number of positive: 6520, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018340 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010252 -> initscore=-4.570005\n",
      "[LightGBM] [Info] Start training from score -4.570005\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015319 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016087 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017134 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.020295 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635992, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569852\n",
      "[LightGBM] [Info] Start training from score -4.569852\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Info] Number of positive: 6520, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015981 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010252 -> initscore=-4.570005\n",
      "[LightGBM] [Info] Start training from score -4.570005\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014810 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018367 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019898 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019530 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635992, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569852\n",
      "[LightGBM] [Info] Start training from score -4.569852\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Info] Number of positive: 6520, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016409 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010252 -> initscore=-4.570005\n",
      "[LightGBM] [Info] Start training from score -4.570005\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014929 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015072 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019916 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018593 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635992, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569852\n",
      "[LightGBM] [Info] Start training from score -4.569852\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Info] Number of positive: 6520, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015111 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010252 -> initscore=-4.570005\n",
      "[LightGBM] [Info] Start training from score -4.570005\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015533 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015509 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016030 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019003 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635992, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569852\n",
      "[LightGBM] [Info] Start training from score -4.569852\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Info] Number of positive: 6520, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015481 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010252 -> initscore=-4.570005\n",
      "[LightGBM] [Info] Start training from score -4.570005\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015928 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018770 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017556 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018699 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635992, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569852\n",
      "[LightGBM] [Info] Start training from score -4.569852\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Info] Number of positive: 6520, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018490 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010252 -> initscore=-4.570005\n",
      "[LightGBM] [Info] Start training from score -4.570005\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016009 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016158 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016932 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018635 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635992, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569852\n",
      "[LightGBM] [Info] Start training from score -4.569852\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Info] Number of positive: 6520, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017903 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010252 -> initscore=-4.570005\n",
      "[LightGBM] [Info] Start training from score -4.570005\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018081 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017476 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018583 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016429 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635992, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569852\n",
      "[LightGBM] [Info] Start training from score -4.569852\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Info] Number of positive: 6520, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015871 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010252 -> initscore=-4.570005\n",
      "[LightGBM] [Info] Start training from score -4.570005\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016650 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014952 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019717 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019272 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635992, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569852\n",
      "[LightGBM] [Info] Start training from score -4.569852\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Info] Number of positive: 6520, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018850 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010252 -> initscore=-4.570005\n",
      "[LightGBM] [Info] Start training from score -4.570005\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015637 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014661 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019879 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018459 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635992, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569852\n",
      "[LightGBM] [Info] Start training from score -4.569852\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Info] Number of positive: 6520, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019193 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010252 -> initscore=-4.570005\n",
      "[LightGBM] [Info] Start training from score -4.570005\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018286 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016214 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018900 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018842 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635992, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569852\n",
      "[LightGBM] [Info] Start training from score -4.569852\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Info] Number of positive: 6520, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016305 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010252 -> initscore=-4.570005\n",
      "[LightGBM] [Info] Start training from score -4.570005\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018225 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016171 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017877 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.020020 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635992, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569852\n",
      "[LightGBM] [Info] Start training from score -4.569852\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Info] Number of positive: 6520, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016202 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010252 -> initscore=-4.570005\n",
      "[LightGBM] [Info] Start training from score -4.570005\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015803 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017730 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018381 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018545 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635992, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569852\n",
      "[LightGBM] [Info] Start training from score -4.569852\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Info] Number of positive: 6520, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014290 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010252 -> initscore=-4.570005\n",
      "[LightGBM] [Info] Start training from score -4.570005\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015286 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016924 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019485 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019392 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635992, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569852\n",
      "[LightGBM] [Info] Start training from score -4.569852\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Info] Number of positive: 6520, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018192 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010252 -> initscore=-4.570005\n",
      "[LightGBM] [Info] Start training from score -4.570005\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016431 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015780 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.020346 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015990 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635992, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569852\n",
      "[LightGBM] [Info] Start training from score -4.569852\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Info] Number of positive: 6520, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016863 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010252 -> initscore=-4.570005\n",
      "[LightGBM] [Info] Start training from score -4.570005\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017997 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015978 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018824 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019222 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635992, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569852\n",
      "[LightGBM] [Info] Start training from score -4.569852\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Info] Number of positive: 6520, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018231 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010252 -> initscore=-4.570005\n",
      "[LightGBM] [Info] Start training from score -4.570005\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016734 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015592 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017845 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015364 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635992, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569852\n",
      "[LightGBM] [Info] Start training from score -4.569852\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Info] Number of positive: 6520, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017834 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010252 -> initscore=-4.570005\n",
      "[LightGBM] [Info] Start training from score -4.570005\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015627 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015535 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017644 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014633 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635992, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569852\n",
      "[LightGBM] [Info] Start training from score -4.569852\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Info] Number of positive: 6520, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.042741 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010252 -> initscore=-4.570005\n",
      "[LightGBM] [Info] Start training from score -4.570005\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018465 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015277 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017467 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015829 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635992, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569852\n",
      "[LightGBM] [Info] Start training from score -4.569852\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 6520, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015801 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010252 -> initscore=-4.570005\n",
      "[LightGBM] [Info] Start training from score -4.570005\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019117 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016936 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018239 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018808 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635992, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569852\n",
      "[LightGBM] [Info] Start training from score -4.569852\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 6520, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017645 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010252 -> initscore=-4.570005\n",
      "[LightGBM] [Info] Start training from score -4.570005\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014535 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013767 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017463 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018304 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635992, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569852\n",
      "[LightGBM] [Info] Start training from score -4.569852\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 6520, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017352 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010252 -> initscore=-4.570005\n",
      "[LightGBM] [Info] Start training from score -4.570005\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016591 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015895 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016502 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015522 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635992, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569852\n",
      "[LightGBM] [Info] Start training from score -4.569852\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 6520, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019990 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010252 -> initscore=-4.570005\n",
      "[LightGBM] [Info] Start training from score -4.570005\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017978 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019055 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019039 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018995 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635992, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569852\n",
      "[LightGBM] [Info] Start training from score -4.569852\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 6520, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016375 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010252 -> initscore=-4.570005\n",
      "[LightGBM] [Info] Start training from score -4.570005\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016182 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014986 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017667 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018030 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635992, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569852\n",
      "[LightGBM] [Info] Start training from score -4.569852\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 6520, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018037 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010252 -> initscore=-4.570005\n",
      "[LightGBM] [Info] Start training from score -4.570005\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016315 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018239 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017634 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016105 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635992, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569852\n",
      "[LightGBM] [Info] Start training from score -4.569852\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 6520, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016397 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010252 -> initscore=-4.570005\n",
      "[LightGBM] [Info] Start training from score -4.570005\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019028 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016908 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.021371 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015118 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635992, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569852\n",
      "[LightGBM] [Info] Start training from score -4.569852\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 6520, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019010 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010252 -> initscore=-4.570005\n",
      "[LightGBM] [Info] Start training from score -4.570005\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018860 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015645 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019441 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015668 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635992, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569852\n",
      "[LightGBM] [Info] Start training from score -4.569852\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 6520, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015635 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010252 -> initscore=-4.570005\n",
      "[LightGBM] [Info] Start training from score -4.570005\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013350 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016415 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629470\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019907 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 635991, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569850\n",
      "[LightGBM] [Info] Start training from score -4.569850\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 6521, number of negative: 629471\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018253 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 635992, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569852\n",
      "[LightGBM] [Info] Start training from score -4.569852\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 8151, number of negative: 786838\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.024617 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 794989, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569882\n",
      "[LightGBM] [Info] Start training from score -4.569882\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 8151, number of negative: 786838\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.024139 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 794989, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569882\n",
      "[LightGBM] [Info] Start training from score -4.569882\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "Model TPR: 0.5486, Model FPR: 0.0499, Model Threshold: 0.0414, Model AUROC: 0.8942\n",
      "Fairness Ratio: 0.2973 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary packages and libraries\n",
    "import pandas as pd\n",
    "import lightgbm as lgbm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import xgboost as xgb\n",
    "import catboost as cb\n",
    "import matplotlib.pyplot as plt\n",
    "from baf import BAFDataset\n",
    "from metrics import calculate_tpr_at_fpr, calculate_fairness_metrics\n",
    "import os\n",
    "import joblib\n",
    "import time\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "# Define your custom metric function\n",
    "def TPR_at_05FPR(y_test, scored_test):\n",
    "    fpr, tpr, threshold = roc_curve(y_test, scored_test)\n",
    "    tpr_at_05fpr = tpr[fpr < 0.05][-1]\n",
    "    return tpr_at_05fpr\n",
    "\n",
    "# Convert your custom metric function to a scorer object\n",
    "custom_scorer = make_scorer(TPR_at_05FPR, greater_is_better=True)\n",
    "\n",
    "# Hyperparameter grids for each model\n",
    "# Here we only run the hyperparameter tuning for LightGBM\n",
    "param_grids = {\n",
    "    \"LightGBM\": {\n",
    "        'num_leaves': [10, 25, 50],\n",
    "        'min_data_in_leaf': [10, 25, 50],\n",
    "        'max_depth': [1, 3, 5],\n",
    "        'n_estimators': [50, 100, 200],\n",
    "    }\n",
    "}\n",
    "\n",
    "# define these dictionaries for storing the results\n",
    "performance_results = {}\n",
    "fairness_results = {}\n",
    "\n",
    "# List of models to evaluate\n",
    "models = {\n",
    "    \"LightGBM\": lgbm.LGBMClassifier(),\n",
    "}\n",
    "\n",
    "# Instantiate the BAF class\n",
    "ds = BAFDataset()\n",
    "\n",
    "# set the path to the dataset and models\n",
    "base_path = \"/ssd003/projects/aieng/public/anomaly_detection_datasets/BAF\"\n",
    "variant = \"Base.csv\"\n",
    "\n",
    "# Add file to path\n",
    "file_path = os.path.join(base_path, variant)\n",
    "\n",
    "# Load the dataset\n",
    "df = ds.load_data(file_path)\n",
    "\n",
    "# Split the dataset into train and test sets for the current variant\n",
    "(X_train, y_train), (X_test, y_test) = ds.train_test_split(df, month=6)\n",
    "\n",
    "# One-hot encode the categorical features in the dataset\n",
    "X_train, X_test = ds.one_hot_encode_categorical(X_train, X_test)\n",
    "\n",
    "# Loop over models\n",
    "for model_name, model in models.items():\n",
    "    print(f\"Hyperparameter tuning for {model_name} on {variant.split('.')[0]}...\")\n",
    "\n",
    "    # Initialize dictionaries to store results\n",
    "    performance_results[(variant.split('.')[0], model_name)] = {}\n",
    "    fairness_results[(variant.split('.')[0], model_name)] = {}\n",
    "\n",
    "    # Perform hyperparameter tuning using GridSearchCV\n",
    "    param_grid = param_grids[model_name]\n",
    "    # scoring should be 'roc_auc' if you want to optimize for AUROC\n",
    "    # Here we want to maximize TPR@5%FPR\n",
    "    search = GridSearchCV(model, param_grid, scoring=custom_scorer, cv=5, verbose=0)\n",
    "    search.fit(X_train, y_train)\n",
    "\n",
    "    # Get the best model with tuned hyperparameters\n",
    "    best_model = search.best_estimator_\n",
    "\n",
    "    # Train the best model\n",
    "    best_model.fit(X_train, y_train)\n",
    "\n",
    "    # Predict on the test set using the best model\n",
    "    scored_test = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # Calculate the TPR@5%FPR on the test set\n",
    "    metrics_dict = calculate_tpr_at_fpr(y_test, scored_test, fpr_lim=0.05)\n",
    "    performance_results[(variant.split('.')[0], model_name)] = metrics_dict\n",
    "    print(f\"Model TPR: {metrics_dict['TPR']}, Model FPR: {metrics_dict['FPR']}, Model Threshold: {metrics_dict['Threshold']}, Model AUROC: {metrics_dict['AUROC']}\")\n",
    "\n",
    "    # Calculate the fairness metrics on the test set\n",
    "    fairness_ratio = calculate_fairness_metrics(y_test, scored_test, X_test, fpr_lim=0.05)\n",
    "    fairness_results[(variant.split('.')[0], model_name)][\"Fairness Ratio\"] = fairness_ratio\n",
    "    print(f\"Fairness Ratio: {fairness_ratio}\", \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fcbf158f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'max_depth': 5, 'min_data_in_leaf': 50, 'n_estimators': 200, 'num_leaves': 10}\n"
     ]
    }
   ],
   "source": [
    "# Access the best hyperparameters\n",
    "best_hyperparameters = search.best_params_\n",
    "print(\"Best Hyperparameters:\", best_hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84339847",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>AUROC</th>\n",
       "      <th>FPR</th>\n",
       "      <th>TPR</th>\n",
       "      <th>Threshold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Base</th>\n",
       "      <th>LightGBM</th>\n",
       "      <td>0.8942</td>\n",
       "      <td>0.0499</td>\n",
       "      <td>0.5486</td>\n",
       "      <td>0.0414</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                AUROC     FPR     TPR  Threshold\n",
       "Base LightGBM  0.8942  0.0499  0.5486     0.0414"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_performance_results = pd.DataFrame(performance_results).transpose()\n",
    "df_performance_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a680d43d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Fairness Ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Base</th>\n",
       "      <th>LightGBM</th>\n",
       "      <td>0.2973</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Fairness Ratio\n",
       "Base LightGBM          0.2973"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fairness_results = pd.DataFrame(fairness_results).transpose()\n",
    "df_fairness_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f45b27c",
   "metadata": {},
   "source": [
    "If you want to try more advanced hyperparameter tuning, you can try [Optuna](https://github.com/optuna/optuna).\n",
    "\n",
    "<ul>\n",
    "    <li>Optuna is an open-source hyperparameter optimization framework.</li><br>\n",
    "    <li>It automates hyperparameter search for machine learning models.</li><br>\n",
    "    <li>Optuna effectively explores the hyperparameter space to find the best configuration for model performance.</li><br>\n",
    "    <li>It integrates seamlessly with popular machine learning libraries, including TensorFlow, Keras, PyTorch, Scikit-learn, etc</li><br>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402f22aa",
   "metadata": {},
   "source": [
    "# Fraud Detection on Other Variants of BAF Dataset\n",
    "\n",
    "This code applies LightGBM to other variants (Variant I, Variant II, Variant III, Variant IV, Variant V) of the BAF dataset for anomaly detection.\n",
    "\n",
    "The code performs the following steps for each dataset variant:\n",
    "\n",
    "1. Loads the BAF dataset variant.\n",
    "2. Splits the dataset into training and test sets.\n",
    "3. Performs one-hot encoding of categorical features.\n",
    "4. Trains and evaluates the the model.\n",
    "5. Store the training and inference time in dictionary\n",
    "6. Calculates the TPR@5%FPR (True Positive Rate at 5% False Positive Rate) on the test set.\n",
    "7. Computes fairness metrics based on the test set predictions.\n",
    "8. Stores the results in a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "eb374fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary packages and libraries\n",
    "import pandas as pd\n",
    "import lightgbm as lgbm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import xgboost as xgb\n",
    "import catboost as cb\n",
    "import matplotlib.pyplot as plt\n",
    "from baf import BAFDataset\n",
    "from metrics import calculate_tpr_at_fpr, calculate_fairness_metrics\n",
    "import os\n",
    "import joblib\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "063638d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data for Variant I...\n",
      "Training and evaluating LightGBM on Variant I...\n",
      "[LightGBM] [Info] Number of positive: 8151, number of negative: 786839\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.172598 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3231\n",
      "[LightGBM] [Info] Number of data points in the train set: 794990, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569883\n",
      "[LightGBM] [Info] Start training from score -4.569883\n",
      "Training Time: 14.001213550567627 seconds\n",
      "Inference Time: 1.135824203491211 seconds\n",
      "Model TPR: 0.5038, Model FPR: 0.0499, Model Threshold: 0.0425, Model AUROC: 0.8842\n",
      "Fairness Ratio: 0.7513 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Loading data for Variant II...\n",
      "Training and evaluating LightGBM on Variant II...\n",
      "[LightGBM] [Info] Number of positive: 8151, number of negative: 786839\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.121246 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 794990, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010253 -> initscore=-4.569883\n",
      "[LightGBM] [Info] Start training from score -4.569883\n",
      "Training Time: 13.761349201202393 seconds\n",
      "Inference Time: 1.0325202941894531 seconds\n",
      "Model TPR: 0.5323, Model FPR: 0.0499, Model Threshold: 0.0408, Model AUROC: 0.8927\n",
      "Fairness Ratio: 0.3139 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Loading data for Variant III...\n",
      "Training and evaluating LightGBM on Variant III...\n",
      "[LightGBM] [Info] Number of positive: 8378, number of negative: 749950\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.120763 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3741\n",
      "[LightGBM] [Info] Number of data points in the train set: 758328, number of used features: 52\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.011048 -> initscore=-4.494397\n",
      "[LightGBM] [Info] Start training from score -4.494397\n",
      "Training Time: 14.377689123153687 seconds\n",
      "Inference Time: 1.2177414894104004 seconds\n",
      "Model TPR: 0.7413, Model FPR: 0.0498, Model Threshold: 0.0254, Model AUROC: 0.9513\n",
      "Fairness Ratio: 0.3445 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Loading data for Variant IV...\n",
      "Training and evaluating LightGBM on Variant IV...\n",
      "[LightGBM] [Info] Number of positive: 8378, number of negative: 749950\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.111425 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3232\n",
      "[LightGBM] [Info] Number of data points in the train set: 758328, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.011048 -> initscore=-4.494397\n",
      "[LightGBM] [Info] Start training from score -4.494397\n",
      "Training Time: 13.079432249069214 seconds\n",
      "Inference Time: 1.2058911323547363 seconds\n",
      "Model TPR: 0.4023, Model FPR: 0.0499, Model Threshold: 0.0431, Model AUROC: 0.8477\n",
      "Fairness Ratio: 0.3227 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Loading data for Variant V...\n",
      "Training and evaluating LightGBM on Variant V...\n",
      "[LightGBM] [Info] Number of positive: 8378, number of negative: 749950\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.121239 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3740\n",
      "[LightGBM] [Info] Number of data points in the train set: 758328, number of used features: 52\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.011048 -> initscore=-4.494397\n",
      "[LightGBM] [Info] Start training from score -4.494397\n",
      "Training Time: 14.449360609054565 seconds\n",
      "Inference Time: 1.207582712173462 seconds\n",
      "Model TPR: 0.3375, Model FPR: 0.0497, Model Threshold: 0.0251, Model AUROC: 0.7824\n",
      "Fairness Ratio: 0.3766 \n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the BAF class\n",
    "ds = BAFDataset()\n",
    "\n",
    "# set the path to the dataset and models\n",
    "base_path = \"/ssd003/projects/aieng/public/anomaly_detection_datasets/BAF\"\n",
    "model_path = \"/ssd003/projects/aieng/public/anomaly_detection_models/BAF\"\n",
    "\n",
    "# define these dictionaries for storing the results\n",
    "performance_results = {}\n",
    "fairness_results = {}\n",
    "\n",
    "# Dictionaries to store training and inference times\n",
    "training_times_dict = {}\n",
    "inference_times_dict = {}\n",
    "\n",
    "# List of variants and their corresponding file names\n",
    "variants = {\n",
    "    'Variant I': 'Variant I.csv',\n",
    "    'Variant II': 'Variant II.csv',\n",
    "    'Variant III': 'Variant III.csv',\n",
    "    'Variant IV': 'Variant IV.csv',\n",
    "    'Variant V': 'Variant V.csv',\n",
    "}\n",
    "\n",
    "# List of models to evaluate\n",
    "models = {\n",
    "    \"LightGBM\": lgbm.LGBMClassifier(),\n",
    "}\n",
    "\n",
    "# Loop over variants and models\n",
    "for variant, file_name in variants.items():\n",
    "    print(f\"Loading data for {variant}...\")\n",
    "    \n",
    "    # Load data from CSV file\n",
    "    df = pd.read_csv(os.path.join(base_path, file_name))\n",
    "\n",
    "    # Split the dataset into train and test sets for the current variant\n",
    "    (X_train, y_train), (X_test, y_test) = ds.train_test_split(df, month=6)\n",
    "\n",
    "    # One-hot encode the categorical features in the dataset\n",
    "    X_train, X_test = ds.one_hot_encode_categorical(X_train, X_test)\n",
    "\n",
    "    # Loop over models\n",
    "    for model_name, model in models.items():\n",
    "        print(f\"Training and evaluating {model_name} on {variant}...\")\n",
    "        \n",
    "        # Initialize dictionaries to store results\n",
    "        training_times_dict[(variant, model_name)] = {}\n",
    "        inference_times_dict[(variant, model_name)] = {}\n",
    "        performance_results[(variant, model_name)] = {}\n",
    "        fairness_results[(variant, model_name)] = {}\n",
    "\n",
    "        # Start measuring training time\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Train the model\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # End measuring training time\n",
    "        end_time = time.time()\n",
    "        training_time = end_time - start_time\n",
    "        print(f\"Training Time: {training_time} seconds\")\n",
    "        training_times_dict[(variant, model_name)][\"Training Time\"] = training_time\n",
    "\n",
    "        # Save the model\n",
    "        path_to_model = os.path.join(model_path, f\"{variant}_{model_name.replace(' ', '_').lower()}_model.joblib\")\n",
    "        joblib.dump(model, path_to_model)\n",
    "\n",
    "        # Start measuring inference time\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Predict on the test set\n",
    "        scored_test = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "        # End measuring inference time\n",
    "        end_time = time.time()\n",
    "        inference_time = end_time - start_time\n",
    "        print(f\"Inference Time: {inference_time} seconds\")\n",
    "        inference_times_dict[(variant, model_name)][\"Inference Time\"] = inference_time\n",
    "\n",
    "        # Calculate the TPR@5%FPR on the test set\n",
    "        metrics_dict = calculate_tpr_at_fpr(y_test, scored_test, fpr_lim=0.05)\n",
    "        print(f\"Model TPR: {metrics_dict['TPR']}, Model FPR: {metrics_dict['FPR']}, Model Threshold: {metrics_dict['Threshold']}, Model AUROC: {metrics_dict['AUROC']}\")\n",
    "        performance_results[(variant, model_name)] = metrics_dict\n",
    "\n",
    "        # Calculate the fairness metrics on the test set\n",
    "        fairness_ratio = calculate_fairness_metrics(y_test, scored_test, X_test, fpr_lim=0.05)\n",
    "        fairness_results[(variant, model_name)][\"Fairness Ratio\"] = fairness_ratio\n",
    "        print(f\"Fairness Ratio: {fairness_ratio}\", \"\\n\")\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfca985",
   "metadata": {},
   "source": [
    "## Result\n",
    "\n",
    "The table below shows all results for LightGBM on other variants of BAF dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "9791a8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame from all results\n",
    "df_performance_results = pd.DataFrame(performance_results).transpose()\n",
    "df_fairness_results = pd.DataFrame(fairness_results).transpose()\n",
    "df_training_times_dict = pd.DataFrame(training_times_dict).transpose()\n",
    "df_inference_times_dict = pd.DataFrame(inference_times_dict).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "2aa08152",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate all results together\n",
    "all_results = pd.concat([df_performance_results, df_fairness_results, df_training_times_dict, df_inference_times_dict], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "24355a09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>TPR</th>\n",
       "      <th>FPR</th>\n",
       "      <th>Threshold</th>\n",
       "      <th>AUROC</th>\n",
       "      <th>Fairness Ratio</th>\n",
       "      <th>Training Time</th>\n",
       "      <th>Inference Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Variant I</th>\n",
       "      <th>LightGBM</th>\n",
       "      <td>0.5038</td>\n",
       "      <td>0.0499</td>\n",
       "      <td>0.0425</td>\n",
       "      <td>0.8842</td>\n",
       "      <td>0.7513</td>\n",
       "      <td>14.001214</td>\n",
       "      <td>1.135824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Variant II</th>\n",
       "      <th>LightGBM</th>\n",
       "      <td>0.5323</td>\n",
       "      <td>0.0499</td>\n",
       "      <td>0.0408</td>\n",
       "      <td>0.8927</td>\n",
       "      <td>0.3139</td>\n",
       "      <td>13.761349</td>\n",
       "      <td>1.032520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Variant III</th>\n",
       "      <th>LightGBM</th>\n",
       "      <td>0.7413</td>\n",
       "      <td>0.0498</td>\n",
       "      <td>0.0254</td>\n",
       "      <td>0.9513</td>\n",
       "      <td>0.3445</td>\n",
       "      <td>14.377689</td>\n",
       "      <td>1.217741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Variant IV</th>\n",
       "      <th>LightGBM</th>\n",
       "      <td>0.4023</td>\n",
       "      <td>0.0499</td>\n",
       "      <td>0.0431</td>\n",
       "      <td>0.8477</td>\n",
       "      <td>0.3227</td>\n",
       "      <td>13.079432</td>\n",
       "      <td>1.205891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Variant V</th>\n",
       "      <th>LightGBM</th>\n",
       "      <td>0.3375</td>\n",
       "      <td>0.0497</td>\n",
       "      <td>0.0251</td>\n",
       "      <td>0.7824</td>\n",
       "      <td>0.3766</td>\n",
       "      <td>14.449361</td>\n",
       "      <td>1.207583</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         TPR     FPR  Threshold   AUROC  Fairness Ratio  \\\n",
       "Variant I   LightGBM  0.5038  0.0499     0.0425  0.8842          0.7513   \n",
       "Variant II  LightGBM  0.5323  0.0499     0.0408  0.8927          0.3139   \n",
       "Variant III LightGBM  0.7413  0.0498     0.0254  0.9513          0.3445   \n",
       "Variant IV  LightGBM  0.4023  0.0499     0.0431  0.8477          0.3227   \n",
       "Variant V   LightGBM  0.3375  0.0497     0.0251  0.7824          0.3766   \n",
       "\n",
       "                      Training Time  Inference Time  \n",
       "Variant I   LightGBM      14.001214        1.135824  \n",
       "Variant II  LightGBM      13.761349        1.032520  \n",
       "Variant III LightGBM      14.377689        1.217741  \n",
       "Variant IV  LightGBM      13.079432        1.205891  \n",
       "Variant V   LightGBM      14.449361        1.207583  "
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ecfdc1",
   "metadata": {},
   "source": [
    "# References:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<ul>\n",
    "    <li>Jesus S, Pombal J, Alves D, Cruz A, Saleiro P, Ribeiro R, Gama J, Bizarro P. Turning the tables: Biased, imbalanced, dynamic tabular datasets for ml evaluation. Advances in Neural Information Processing Systems. 2022 Dec 6;35:33563-75.</li>\n",
    "\n",
    "</ul>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ad",
   "language": "python",
   "name": "ad"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "10259be683146ba0cfb84d8d370984231ebebae69a5ceae05aa739bef317bea6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
